"""
ì¼ì¼ ë³´ê³ ì„œ ìë™ Ingestion íŒŒì´í”„ë¼ì¸

backend/Data/mock_reports/ ì•„ë˜ ëª¨ë“  txt íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ìŠ¤ìº”í•˜ì—¬
ê° txt íŒŒì¼ì˜ ì—¬ëŸ¬ JSON ê°ì²´ë¥¼ íŒŒì‹± â†’ normalize â†’ chunk â†’ embed â†’ Chroma ì €ì¥

ì‚¬ìš©ë²•:
    python -m ingestion.ingest_daily_reports
    
    ë˜ëŠ” OpenAI API í‚¤ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬:
    python -m ingestion.ingest_daily_reports --api-key YOUR_API_KEY
"""
import os
import sys
import json
import re
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    env_path = project_root / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… .env íŒŒì¼ ë¡œë“œë¨: {env_path}")
    else:
        print(f"âš ï¸  .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {env_path}")
except ImportError:
    print("âš ï¸  python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-dotenvë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
except Exception as e:
    print(f"âš ï¸  .env íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")

from app.domain.report.service import ReportProcessingService
from app.domain.report.chunker import chunk_report, get_chunk_statistics
from ingestion.embed import embed_texts
from ingestion.chroma_client import get_chroma_service


# ========================================
# ì„¤ì •
# ========================================
DATA_DIR = project_root / "Data" / "mock_reports"
COLLECTION_NAME = "daily_reports"
BATCH_SIZE = 100


# ========================================
# JSON íŒŒì‹± í•¨ìˆ˜
# ========================================
def parse_multi_json_file(file_path: Path) -> List[Dict[str, Any]]:
    """
    txt íŒŒì¼ì—ì„œ ì—¬ëŸ¬ ê°œì˜ JSON ê°ì²´ë¥¼ íŒŒì‹±
    
    ê° JSON ê°ì²´ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œë§Œ êµ¬ë¶„ë˜ì–´ ìˆìŒ (ë°°ì—´ì´ ì•„ë‹˜)
    
    Args:
        file_path: txt íŒŒì¼ ê²½ë¡œ
        
    Returns:
        íŒŒì‹±ëœ JSON ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ë°©ë²• 1: ì •ê·œì‹ìœ¼ë¡œ JSON ê°ì²´ ë¸”ë¡ ì¶”ì¶œ
    # {ë¡œ ì‹œì‘í•˜ê³  }ë¡œ ëë‚˜ëŠ” íŒ¨í„´ì„ ì°¾ìŒ
    json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
    json_strings = re.findall(json_pattern, content, re.DOTALL)
    
    parsed_objects = []
    
    for idx, json_str in enumerate(json_strings):
        try:
            obj = json.loads(json_str)
            parsed_objects.append(obj)
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {idx}): {e}")
            # ì‹¤íŒ¨í•œ JSONì€ ê±´ë„ˆë›°ê¸°
            continue
    
    return parsed_objects


def parse_month_folder(month_str: str) -> tuple:
    """
    ì›” í´ë” ì´ë¦„ì„ íŒŒì‹±í•˜ì—¬ (year, month) íŠœí”Œ ë°˜í™˜
    
    ì˜ˆ: "2024ë…„ 11ì›”" -> (2024, 11)
        "2025ë…„ 1ì›”" -> (2025, 1)
    """
    try:
        # "2024ë…„ 11ì›”" í˜•ì‹ íŒŒì‹±
        parts = month_str.replace("ë…„", "").replace("ì›”", "").strip().split()
        if len(parts) >= 2:
            year = int(parts[0])
            month = int(parts[1])
            return (year, month)
    except:
        pass
    return (0, 0)


def parse_file_date(file_name: str, month_folder: str = "") -> str:
    """
    íŒŒì¼ ì´ë¦„ì—ì„œ ì‹œì‘ ë‚ ì§œ ì¶”ì¶œ
    
    ì˜ˆ: "2024-11-01 ~ 2024-11-05.txt" -> "2024-11-01"
        "2025ë…„ 12ì›” 1ì¼ ~ 12ì›” 5ì¼.txt" (month_folder="2025ë…„ 12ì›”") -> "2025-12-01"
    """
    # "2024-11-01 ~ 2024-11-05.txt" í˜•ì‹
    if " ~ " in file_name:
        date_part = file_name.split(" ~ ")[0].strip()
        # .txt ì œê±°
        date_part = date_part.replace(".txt", "").strip()
        # YYYY-MM-DD í˜•ì‹ì¸ì§€ í™•ì¸
        if re.match(r'^\d{4}-\d{2}-\d{2}$', date_part):
            return date_part
        
        # "2025ë…„ 12ì›” 1ì¼" í˜•ì‹ íŒŒì‹±
        # í´ë”ì—ì„œ ì—°ë„ì™€ ì›” ì¶”ì¶œ
        if month_folder:
            year_month = parse_month_folder(month_folder)
            if year_month[0] > 0 and year_month[1] > 0:
                # "1ì¼", "12ì¼" ë“±ì—ì„œ ì¼ ì¶”ì¶œ
                day_match = re.search(r'(\d+)ì¼', date_part)
                if day_match:
                    day = int(day_match.group(1))
                    return f"{year_month[0]:04d}-{year_month[1]:02d}-{day:02d}"
    
    return ""


def scan_mock_reports(base_dir: Path) -> List[Dict[str, Any]]:
    """
    mock_reports í´ë”ì˜ ëª¨ë“  txt íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ìŠ¤ìº”í•˜ê³  ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬
    
    Args:
        base_dir: mock_reports í´ë” ê²½ë¡œ
        
    Returns:
        íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬ë¨)
        [
            {
                "file_path": Path(...),
                "relative_path": "2024ë…„ 11ì›”/2024-11-01 ~ 2024-11-05.txt",
                "month": "2024ë…„ 11ì›”"
            },
            ...
        ]
    """
    if not base_dir.exists():
        print(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        return []
    
    file_infos = []
    
    # ëª¨ë“  txt íŒŒì¼ ì°¾ê¸°
    for txt_file in base_dir.rglob("*.txt"):
        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
        relative_path = txt_file.relative_to(base_dir)
        
        # ì›” í´ë” ì´ë¦„ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¶€ëª¨ í´ë”)
        month = relative_path.parts[0] if len(relative_path.parts) > 1 else ""
        file_name = relative_path.parts[-1] if len(relative_path.parts) > 0 else ""
        
        # ì •ë ¬ì„ ìœ„í•œ í‚¤ ìƒì„±
        year_month = parse_month_folder(month)
        file_date = parse_file_date(file_name, month)
        
        # ì •ë ¬ í‚¤: (year, month, file_date, relative_path)
        # file_dateê°€ ì—†ìœ¼ë©´ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì •ë ¬
        sort_key = (
            year_month[0],  # year
            year_month[1],  # month
            file_date if file_date else file_name,  # date or filename
            str(relative_path)  # fallback to path
        )
        
        file_infos.append({
            "file_path": txt_file,
            "relative_path": str(relative_path),
            "month": month,
            "sort_key": sort_key
        })
    
    # ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬
    file_infos.sort(key=lambda x: x["sort_key"])
    
    # sort_key ì œê±° (ë°˜í™˜ê°’ì—ëŠ” í•„ìš” ì—†ìŒ)
    for info in file_infos:
        del info["sort_key"]
    
    return file_infos


# ========================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ========================================
def ingest_daily_reports_pipeline(api_key: str = None, dry_run: bool = False):
    """
    ì¼ì¼ ë³´ê³ ì„œ ì „ì²´ Ingestion íŒŒì´í”„ë¼ì¸
    
    Args:
        api_key: OpenAI API í‚¤
        dry_run: Trueë©´ Chroma ì—…ë¡œë“œ ì—†ì´ í†µê³„ë§Œ ì¶œë ¥
    """
    print("=" * 80)
    print("ğŸ“Š ì¼ì¼ ë³´ê³ ì„œ Ingestion íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 80)
    print()
    
    # 1. íŒŒì¼ ìŠ¤ìº”
    print("â³ mock_reports í´ë” ìŠ¤ìº” ì¤‘...")
    file_infos = scan_mock_reports(DATA_DIR)
    
    if not file_infos:
        print("âŒ txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì´ {len(file_infos)}ê°œ txt íŒŒì¼ ë°œê²¬ (ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬ë¨)")
    print()
    
    # ìŠ¤ìº”ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 5ê°œ, ë§ˆì§€ë§‰ 5ê°œ)
    if len(file_infos) > 0:
        print("ğŸ“‹ ìŠ¤ìº”ëœ íŒŒì¼ ëª©ë¡ (ì²˜ìŒ 5ê°œ):")
        for i, file_info in enumerate(file_infos[:5]):
            print(f"   {i+1}. {file_info['relative_path']}")
        if len(file_infos) > 10:
            print("   ...")
            print("ğŸ“‹ ìŠ¤ìº”ëœ íŒŒì¼ ëª©ë¡ (ë§ˆì§€ë§‰ 5ê°œ):")
            for i, file_info in enumerate(file_infos[-5:], start=len(file_infos)-4):
                print(f"   {i}. {file_info['relative_path']}")
        print()
    
    # 2. ReportProcessingService ì´ˆê¸°í™” (dry-runì—ì„œëŠ” API í‚¤ ì—†ì´ ìƒì„±)
    # normalize í•¨ìˆ˜ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ OpenAI í´ë¼ì´ì–¸íŠ¸ëŠ” í•„ìš” ì—†ìŒ
    if dry_run:
        # API í‚¤ ì—†ì´ ìƒì„± (normalizeë§Œ ì‚¬ìš©)
        service = ReportProcessingService.__new__(ReportProcessingService)
        # normalize ë©”ì„œë“œë“¤ë§Œ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ clientëŠ” Noneìœ¼ë¡œ ì„¤ì •
        service.client = None
    else:
        service = ReportProcessingService(api_key=api_key)
    
    # 3. ì „ì²´ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ëª¨ë“  íŒŒì¼ì˜ ì²­í¬ë¥¼ ëª¨ìŒ)
    all_chunks = []
    stats = {
        "total_files": len(file_infos),
        "total_reports": 0,
        "total_chunks": 0,
        "errors": []
    }
    
    # 4. ê° txt íŒŒì¼ ì²˜ë¦¬
    current_folder = None
    for idx, file_info in enumerate(file_infos):
        file_path = file_info["file_path"]
        relative_path = file_info["relative_path"]
        month = file_info["month"]
        
        # í´ë”ê°€ ë³€ê²½ë˜ë©´ ë¡œê·¸ ì¶œë ¥
        if month != current_folder:
            current_folder = month
            print("=" * 80)
            print(f"ğŸ“‚ Processing folder: {month}")
            print("=" * 80)
        
        print("-" * 80)
        print(f"ğŸ“„ Processing file [{idx + 1}/{len(file_infos)}]: {relative_path}")
        print(f"   Folder: {month}")
        print("-" * 80)
        
        try:
            # 4-1. JSON ê°ì²´ë“¤ íŒŒì‹±
            print("â³ JSON íŒŒì‹± ì¤‘...")
            json_objects = parse_multi_json_file(file_path)
            
            if not json_objects:
                print(f"âš ï¸  íŒŒì‹±ëœ JSON ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                stats["errors"].append({
                    "file": relative_path,
                    "error": "No JSON objects parsed"
                })
                continue
            
            print(f"âœ… {len(json_objects)}ê°œ JSON ê°ì²´ íŒŒì‹± ì™„ë£Œ")
            stats["total_reports"] += len(json_objects)
            
            # 4-2. ê° JSON ê°ì²´ë¥¼ Canonicalë¡œ ë³€í™˜ + ì²­í‚¹
            for json_idx, raw_json in enumerate(json_objects):
                try:
                    # Normalize (Raw JSON â†’ CanonicalReport)
                    canonical = service.normalize_daily(raw_json)
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„°ìš©)
                    date_str = canonical.period_start.isoformat() if canonical.period_start else ""
                    month_str = canonical.period_start.strftime("%Y-%m") if canonical.period_start else ""
                    
                    # Chunking
                    chunks = chunk_report(canonical, include_summary=True)
                    
                    # ê° ì²­í¬ì— ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    for chunk in chunks:
                        # ì²­í¬ ë”•ì…”ë„ˆë¦¬ í‚¤ ì´ë¦„ ë³€ê²½ (text â†’ chunk_text)
                        chunk["chunk_text"] = chunk.pop("text")
                        
                        # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                        chunk["metadata"]["date"] = date_str
                        chunk["metadata"]["month"] = month_str
                        chunk["metadata"]["source_file"] = relative_path
                        chunk["metadata"]["task_count"] = len(canonical.tasks)
                        chunk["metadata"]["issue_count"] = len(canonical.issues)
                        chunk["metadata"]["plan_count"] = len(canonical.plans)
                    
                    all_chunks.extend(chunks)
                    print(f"  âœ… ë³´ê³ ì„œ {json_idx + 1}: {len(chunks)}ê°œ ì²­í¬ ìƒì„± (ì‘ì„±ì¼: {date_str}, ì‘ì„±ì: {canonical.owner})")
                
                except Exception as e:
                    print(f"  âŒ ë³´ê³ ì„œ {json_idx + 1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    stats["errors"].append({
                        "file": relative_path,
                        "json_index": json_idx,
                        "error": str(e)
                    })
        
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            stats["errors"].append({
                "file": relative_path,
                "error": str(e)
            })
    
    stats["total_chunks"] = len(all_chunks)
    
    print()
    print("=" * 80)
    print("ğŸ“Š íŒŒì‹± ë° ì²­í‚¹ í†µê³„")
    print("=" * 80)
    print(f"ì´ íŒŒì¼ ìˆ˜: {stats['total_files']}")
    print(f"ì´ ë³´ê³ ì„œ ìˆ˜: {stats['total_reports']}")
    print(f"ì´ ì²­í¬ ìˆ˜: {stats['total_chunks']}")
    print(f"ì˜¤ë¥˜ ìˆ˜: {len(stats['errors'])}")
    print()
    
    if stats["errors"]:
        print("âš ï¸  ì˜¤ë¥˜ ëª©ë¡:")
        for error in stats["errors"][:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            print(f"  - {error}")
        if len(stats["errors"]) > 10:
            print(f"  ... ì™¸ {len(stats['errors']) - 10}ê±´")
        print()
    
    if not all_chunks:
        print("âŒ ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ì²­í¬ í†µê³„
    chunk_stats = get_chunk_statistics(all_chunks)
    print("ğŸ“Š ì²­í¬ í†µê³„:")
    print(f"  - ì´ ì²­í¬ ìˆ˜: {chunk_stats['total_chunks']}")
    print(f"  - ì²­í¬ íƒ€ì…ë³„:")
    for chunk_type, count in chunk_stats["chunk_types"].items():
        print(f"    â€¢ {chunk_type}: {count}")
    print(f"  - í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {chunk_stats['avg_text_length']:.1f}ì")
    print(f"  - ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´: {chunk_stats['max_text_length']}ì")
    print(f"  - ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´: {chunk_stats['min_text_length']}ì")
    print()
    
    if dry_run:
        print("ğŸ” Dry-run ëª¨ë“œ: Chroma ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # 5. ì„ë² ë”© ìƒì„±
    print("=" * 80)
    print("â³ ì„ë² ë”© ìƒì„± ì¤‘...")
    print("=" * 80)
    
    ids = [chunk["id"] for chunk in all_chunks]
    texts = [chunk["chunk_text"] for chunk in all_chunks]
    metadatas = [chunk["metadata"] for chunk in all_chunks]
    
    try:
        embeddings = embed_texts(texts, api_key=api_key, batch_size=BATCH_SIZE)
        print(f"âœ… {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        print()
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return
    
    # 6. Chroma Cloud ì—…ë¡œë“œ
    print("=" * 80)
    print("â³ Chroma Cloud ì—…ë¡œë“œ ì¤‘...")
    print("=" * 80)
    
    try:
        # Chroma í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        chroma_service = get_chroma_service()
        collection = chroma_service.get_or_create_collection(name=COLLECTION_NAME)
        
        print(f"âœ… ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì—°ê²° ì™„ë£Œ")
        print(f"ğŸ“¦ í˜„ì¬ ë¬¸ì„œ ìˆ˜: {collection.count()}ê°œ")
        print()
        
        # ë°°ì¹˜ ì—…ë¡œë“œ
        total = len(all_chunks)
        
        for i in range(0, total, BATCH_SIZE):
            batch_end = min(i + BATCH_SIZE, total)
            
            batch_ids = ids[i:batch_end]
            batch_embeddings = embeddings[i:batch_end]
            batch_documents = texts[i:batch_end]
            batch_metadatas = metadatas[i:batch_end]
            
            print(f"  â³ ì—…ë¡œë“œ ì¤‘... ({i + 1}-{batch_end}/{total})")
            
            try:
                collection.upsert(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_documents,
                    metadatas=batch_metadatas
                )
            except Exception as e:
                print(f"  âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì˜¤ë¥˜ ({i}-{batch_end}): {e}")
                return
        
        print()
        print("=" * 80)
        print("âœ… Ingestion ì™„ë£Œ!")
        print("=" * 80)
        print(f"ì»¬ë ‰ì…˜: {COLLECTION_NAME}")
        print(f"ì—…ë¡œë“œëœ ì²­í¬: {total}ê°œ")
        print(f"ì»¬ë ‰ì…˜ ì´ ë¬¸ì„œ ìˆ˜: {collection.count()}ê°œ")
        print()
        
    except Exception as e:
        print(f"âŒ Chroma Cloud ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        return


# ========================================
# CLI ì§„ì…ì 
# ========================================
def main():
    """CLI ì§„ì…ì """
    parser = argparse.ArgumentParser(
        description="ì¼ì¼ ë³´ê³ ì„œ ìë™ Ingestion íŒŒì´í”„ë¼ì¸"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        default=None,
        help="OpenAI API í‚¤ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Dry-run ëª¨ë“œ (Chroma ì—…ë¡œë“œ ì—†ì´ í†µê³„ë§Œ ì¶œë ¥)"
    )
    
    args = parser.parse_args()
    
    # API í‚¤ í™•ì¸
    api_key = args.api_key or os.getenv("OPENAI_API_KEY")
    if not api_key and not args.dry_run:
        print("âŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   --api-key ì˜µì…˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    ingest_daily_reports_pipeline(api_key=api_key, dry_run=args.dry_run)


if __name__ == "__main__":
    main()

