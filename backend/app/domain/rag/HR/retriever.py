"""
RAG ê²€ìƒ‰ ëª¨ë“ˆ (LangChain ê¸°ë°˜)

LangChain ì²´ì¸ê³¼ LangSmithë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

from typing import List, Optional, Dict, Any
import time
import os
import json
import datetime

# LangChain imports
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI

# LangSmith ì„¤ì •
from langsmith import traceable

from .config import rag_config
from .vector_store import VectorStore
from .schemas import QueryRequest, QueryResponse, RetrievedChunk
from .utils import get_logger
from .evaluator import RAGEvaluator

logger = get_logger(__name__)


class RAGRetriever:
    """RAG ê¸°ë°˜ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± (LangChain ì²´ì¸ ì‚¬ìš©)"""
    
    def __init__(self, collection_name: Optional[str] = None):
        self.config = rag_config
        self.vector_store = VectorStore(collection_name)
        
        # LangSmith ì„¤ì •
        if self.config.LANGSMITH_TRACING and self.config.LANGSMITH_API_KEY:
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_API_KEY"] = self.config.LANGSMITH_API_KEY
            os.environ["LANGCHAIN_PROJECT"] = self.config.LANGSMITH_PROJECT
            logger.info(f"LangSmith ì¶”ì  í™œì„±í™”: {self.config.LANGSMITH_PROJECT}")
        
        # Lazy loading: LLMì„ ì‹¤ì œ ì‚¬ìš© ì‹œì—ë§Œ ë¡œë“œ
        self._llm = None
        self._rag_chain = None
        self._rag_chain = None
        self._smalltalk_chain = None
        self._evaluator = None
        
        logger.info("RAGRetriever ì´ˆê¸°í™” ì™„ë£Œ (LLM lazy loading)")

    @property
    def evaluator(self):
        """Evaluator lazy loading"""
        if self._evaluator is None:
            self._evaluator = RAGEvaluator()
        return self._evaluator
    
    @property
    def llm(self):
        """LLM lazy loading"""
        if self._llm is None:
            self._llm = ChatOpenAI(
                model=self.config.OPENAI_MODEL,
                temperature=self.config.OPENAI_TEMPERATURE,
                max_tokens=self.config.OPENAI_MAX_TOKENS,
                api_key=self.config.OPENAI_API_KEY
            )
            logger.info(f"LLM ë¡œë“œ ì™„ë£Œ: {self.config.OPENAI_MODEL}")
        return self._llm
    
    @property
    def prompt_template(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        return ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ê·œì¹™ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

1. **ë‚´ìš© ê¸°ë°˜**: ì˜¤ì§ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ë©°, ì¶”ì¸¡í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
2. **Markdown í•„ìˆ˜**: ê°€ë…ì„±ì„ ìœ„í•´ Markdownì„ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
   - **ëª¨ë“  ëª©ë¡(ê¸€ë¨¸ë¦¬ ê¸°í˜¸)ê³¼ ì†Œì œëª©(`###`) ì•ë’¤ì—ëŠ” ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆ ë¬¸ì(\\n)ë¥¼ ë‘ ë²ˆ ì‚¬ìš©í•˜ì—¬ ë¹ˆ ì¤„ì„ ë§Œë“œì„¸ìš”.** (ë§¤ìš° ì¤‘ìš”)
   - í•µì‹¬ ë‚´ìš©ì€ **ë³¼ë“œì²´**ë¡œ ê°•ì¡°í•©ë‹ˆë‹¤.
3. **ê°„ê²°ì„±**:
   - ë¶ˆí•„ìš”í•œ ë¹ˆ ì¤„(3ì¤„ ì´ìƒ ì—°ì†)ì€ í”¼í•˜ë˜, **ê°€ë…ì„±ê³¼ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ì„ ìœ„í•´ í•„ìš”í•œ ì¤„ë°”ê¿ˆì€ ì•„ë¼ì§€ ë§ˆì‹­ì‹œì˜¤.**
   - ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
4. **ì–¸ì–´**: í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""),
            ("user", """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

{context}

ì§ˆë¬¸: {query}

ë‹µë³€:""")
        ])
    
    @property
    def smalltalk_prompt(self):
        """Small talk í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        return ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì¼ìƒì ì¸ ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ëˆ„ê³ , ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""),
            ("user", "{query}")
        ])
    
    @property
    def rag_chain(self):
        """RAG ì²´ì¸ lazy loading"""
        if self._rag_chain is None:
            self._rag_chain = self._build_rag_chain()
            logger.info("RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ")
        return self._rag_chain
    
    @property
    def smalltalk_chain(self):
        """Small talk ì²´ì¸ lazy loading"""
        if self._smalltalk_chain is None:
            self._smalltalk_chain = (
                self.smalltalk_prompt
                | self.llm
                | StrOutputParser()
            )
            logger.info("Small talk ì²´ì¸ êµ¬ì„± ì™„ë£Œ")
        return self._smalltalk_chain
    
    def _build_rag_chain(self):
        """LangChain íŒŒì´í”„ ì—°ì‚°ì(|)ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ êµ¬ì„±"""
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° ë™ì  threshold í•„í„°ë§
        @traceable(name="retrieve_and_filter")
        def retrieve_and_filter(inputs: Dict[str, Any]) -> Dict[str, Any]:
            """ë¬¸ì„œ ê²€ìƒ‰ ë° ë™ì  threshold ê¸°ë°˜ í•„í„°ë§"""
            query = inputs["query"]
            top_k = inputs.get("top_k", self.config.RAG_TOP_K)
            
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘: '{query}' (Top-{top_k})")
            
            # ë²¡í„° ê²€ìƒ‰ (ë” ë§ì´ ê²€ìƒ‰í•˜ì—¬ ë™ì  threshold ì ìš©)
            results = self.vector_store.search(query, top_k * 3)
            
            # ê²°ê³¼ ë³€í™˜ ë° ë™ì  threshold í•„í„°ë§
            retrieved_chunks = []
            all_similarities = []
            
            # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
            if not results:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            elif not results.get('documents') or not results['documents']:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            elif not results['documents'][0]:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            else:
                doc_list = results['documents'][0]
                similarity_list = results.get('distances', [[]])[0] if results.get('distances') else []
                meta_list = results.get('metadatas', [[]])[0] if results.get('metadatas') else []
                
                logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {len(doc_list)}ê°œ ë¬¸ì„œ, {len(similarity_list)}ê°œ ìœ ì‚¬ë„ ì ìˆ˜")
                
                # ëª¨ë“  ìœ ì‚¬ë„ë¥¼ ìˆ˜ì§‘
                for i in range(len(doc_list)):
                    if i < len(similarity_list):
                        similarity_score = float(similarity_list[i])
                    else:
                        similarity_score = 0.0
                    
                    all_similarities.append(similarity_score)
                    
                    metadata = meta_list[i] if i < len(meta_list) else {}
                    chunk = RetrievedChunk(
                        text=doc_list[i],
                        metadata=metadata,
                        score=similarity_score
                    )
                    retrieved_chunks.append(chunk)
            
            # ë™ì  threshold ê³„ì‚°
            if all_similarities:
                # ìµœê³  ì ìˆ˜ì™€ í‰ê·  ì ìˆ˜ ê³„ì‚°
                max_similarity = max(all_similarities)
                avg_similarity = sum(all_similarities) / len(all_similarities)
                
                # ë™ì  threshold: ìµœê³  ì ìˆ˜ì™€ í‰ê· ì˜ ì¤‘ê°„ê°’, min~max ë²”ìœ„ ë‚´ë¡œ ì œí•œ
                dynamic_threshold = (max_similarity + avg_similarity) / 2
                dynamic_threshold = max(
                    self.config.RAG_MIN_SIMILARITY_THRESHOLD,
                    min(dynamic_threshold, self.config.RAG_MAX_SIMILARITY_THRESHOLD)
                )
                
                logger.info(f"ë™ì  threshold ê³„ì‚°: max={max_similarity:.4f}, avg={avg_similarity:.4f}, "
                           f"threshold={dynamic_threshold:.4f} (ë²”ìœ„: {self.config.RAG_MIN_SIMILARITY_THRESHOLD}~{self.config.RAG_MAX_SIMILARITY_THRESHOLD})")
            else:
                dynamic_threshold = self.config.RAG_MIN_SIMILARITY_THRESHOLD
                logger.warning(f"ìœ ì‚¬ë„ ì—†ìŒ, ê¸°ë³¸ threshold ì‚¬ìš©: {dynamic_threshold}")
            
            # ë™ì  threshold ê¸°ë°˜ í•„í„°ë§
            filtered_chunks = []
            for chunk in retrieved_chunks:
                filename = chunk.metadata.get('filename', 'Unknown')
                page_num = chunk.metadata.get('page_number', '?')
                
                if chunk.score >= dynamic_threshold:
                    filtered_chunks.append(chunk)
                    logger.info(f"  âœ“ íŒŒì¼: {filename}, í˜ì´ì§€: {page_num}, ìœ ì‚¬ë„: {chunk.score:.4f} >= {dynamic_threshold:.4f}")
                else:
                    # logger.info(f"  âœ— í•„í„°ë§: {filename}, í˜ì´ì§€: {page_num}, ìœ ì‚¬ë„: {chunk.score:.4f} < {dynamic_threshold:.4f}")
                    pass
            
            # ìƒìœ„ kê°œë§Œ ì„ íƒ
            retrieved_chunks = filtered_chunks[:top_k]
            
            logger.info(f"{len(retrieved_chunks)}ê°œ ì²­í¬ ìµœì¢… ì„ íƒ (ë™ì  threshold: {dynamic_threshold:.4f})")
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            for i, chunk in enumerate(retrieved_chunks, 1):
                context_parts.append(f"[ë¬¸ì„œ {i}]")
                context_parts.append(f"íŒŒì¼: {chunk.metadata.get('filename', 'Unknown')}")
                context_parts.append(f"í˜ì´ì§€: {chunk.metadata.get('page_number', 'Unknown')}")
                context_parts.append(f"ë‚´ìš©:\n{chunk.text}")
                context_parts.append("")
            
            context = "\n".join(context_parts) if context_parts else "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return {
                "query": query,
                "context": context,
                "retrieved_chunks": retrieved_chunks,
                "top_k": top_k,
                "dynamic_threshold": dynamic_threshold
            }
        
        # 2. ë‹µë³€ ìƒì„±
        @traceable(name="generate_answer")
        def generate_answer(inputs: Dict[str, Any]) -> Dict[str, Any]:
            """LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
            query = inputs["query"]
            context = inputs["context"]
            retrieved_chunks = inputs["retrieved_chunks"]
            
            if not retrieved_chunks:
                logger.warning("ê²€ìƒ‰ëœ ì²­í¬ê°€ ì—†ìŒ - ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜")
                return {
                    **inputs,
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # LangChain ì²´ì¸ ì‹¤í–‰: prompt | llm | parser
            answer = (
                self.prompt_template 
                | self.llm 
                | StrOutputParser()
            ).invoke({
                "query": query,
                "context": context
            })
            
            return {
                **inputs,
                "answer": answer
            }
        
        # LangChain ì²´ì¸ êµ¬ì„± (íŒŒì´í”„ ì—°ì‚°ì ì‚¬ìš©)
        chain = (
            RunnablePassthrough()
            | RunnableLambda(retrieve_and_filter)
            | RunnableLambda(generate_answer)
        )
        
        return chain
    
    def needs_search(self, query: str) -> bool:
        """
        ì§ˆë¬¸ì´ ë¬¸ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            bool: ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ True, Small talkì´ë©´ False
        """
        # Small talk í‚¤ì›Œë“œ (ê²€ìƒ‰ ë¶ˆí•„ìš”)
        smalltalk_keywords = [
            "ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "ë°˜ê°€ì›Œ", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ê³ ë§ˆ",
            "ë‚ ì”¨", "ì˜¤ëŠ˜", "ë‚´ì¼", "ì–´ì œ", "ì‹œê°„", "ëª‡ ì‹œ",
            "ê¸°ë¶„", "ì¢‹ì•„", "ì‹«ì–´", "í–‰ë³µ", "ìŠ¬í¼", "í™”ë‚˜",
            "ì˜ ì§€ë‚´", "ì–´ë–»ê²Œ ì§€ë‚´", "ë­í•´", "ë­í•˜ë‹ˆ",
            "ë†€ì", "ì¬ë¯¸", "ì¦ê±°", "ì¦ê±°ì›Œ",
            "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ë¯¸ì•ˆ", "ì£„ì†¡",
            "ì¢‹ì€ í•˜ë£¨", "ì¢‹ì€ ë°¤", "ì˜ ì", "ì•ˆë…•íˆ",
            "ë­ì•¼", "ê·¸ê²Œ ë­ì•¼", "ë¬´ì—‡", "ë­”ê°€",
            "ì¬ë°Œ", "ì›ƒê²¨", "í•˜í•˜", "í—¤í—¤"
        ]
        
        # ë¬¸ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œ í‚¤ì›Œë“œ
        search_keywords = [
            "ê·œì •", "ì •ì±…", "ê·œì¹™", "ê·œì¹™", "ê°€ì´ë“œ", "ë§¤ë‰´ì–¼", "ë§¤ë‰´ì–¼",
            "ì—°ì°¨", "íœ´ê°€", "ê¸‰ì—¬", "ë³µì§€", "í˜œíƒ", "ì§€ì›",
            "ì ˆì°¨", "í”„ë¡œì„¸ìŠ¤", "ë°©ë²•", "ì–´ë–»ê²Œ", "ë¬´ì—‡",
            "íšŒì‚¬", "ì‚¬ë‚´", "ë‚´ë¶€", "ë¬¸ì„œ", "ìë£Œ",
            "ì°¾ì•„", "ê²€ìƒ‰", "ì•Œë ¤ì¤˜", "ì•Œë ¤", "ë§í•´ì¤˜", "ë§í•´",
            "í™•ì¸", "ì¡°íšŒ", "ì¡°íšŒí•´", "ë³´ì—¬ì¤˜", "ë³´ì—¬",
            "ì„¤ëª…", "ì„¤ëª…í•´", "ì´í•´", "ì´í•´í•´"
        ]
        
        query_lower = query.lower()
        
        # Small talk í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆê³  ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ Small talk
        has_smalltalk = any(keyword in query_lower for keyword in smalltalk_keywords)
        has_search = any(keyword in query_lower for keyword in search_keywords)
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ê²€ìƒ‰ í•„ìš”
        if has_search:
            return True
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ê³  Small talk í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ Small talk
        if has_smalltalk and not has_search:
            return False
        
        # LLMìœ¼ë¡œ ë” ì •í™•í•˜ê²Œ íŒë‹¨
        try:
            from langchain_core.messages import SystemMessage, HumanMessage
            
            classification_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì´ íšŒì‚¬ ë¬¸ì„œë‚˜ ê·œì •ì„ ê²€ìƒ‰í•´ì•¼ í•˜ëŠ” ì§ˆë¬¸ì¸ì§€, ì•„ë‹ˆë©´ ì¼ìƒì ì¸ ëŒ€í™”(Small talk)ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

íšŒì‚¬ ë¬¸ì„œ/ê·œì • ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°: "SEARCH"
ì¼ìƒì ì¸ ëŒ€í™”ì¸ ê²½ìš°: "SMALLTALK"

ë‹µë³€ (SEARCH ë˜ëŠ” SMALLTALKë§Œ):"""
            
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤. SEARCH ë˜ëŠ” SMALLTALKë§Œ ë°˜í™˜í•˜ì„¸ìš”."),
                HumanMessage(content=classification_prompt)
            ]
            
            response = self.llm.invoke(messages)
            result = response.content.strip().upper()
            needs_search = result == "SEARCH"
            
            logger.info(f"ì§ˆë¬¸ ë¶„ë¥˜: '{query}' -> {'RAG ê²€ìƒ‰' if needs_search else 'Small talk'}")
            return needs_search
            
        except Exception as e:
            logger.warning(f"ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ìœ¼ë¡œ RAG ê²€ìƒ‰ ì‚¬ìš©")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ RAG ê²€ìƒ‰ ì‚¬ìš©
            return True
    
    @traceable(name="smalltalk_query")  # LangSmith ì¶”ì 
    def query_smalltalk(self, query: str) -> str:
        """
        Small talk ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (LLMë§Œ ì‚¬ìš©)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        try:
            answer = self.smalltalk_chain.invoke({"query": query})
            return answer
        except Exception as e:
            logger.error(f"Small talk ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    @traceable(
        name="rag_query_full",
        metadata={
            "component": "RAG System",
            "version": "1.0"
        }
    )
    def query(self, request: QueryRequest) -> QueryResponse:
        """
        ì§ˆì˜ì‘ë‹µ ì „ì²´ í”„ë¡œì„¸ìŠ¤ (ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ RAG ë˜ëŠ” LLM ë‹¨ë… ì‚¬ìš©)
        
        Args:
            request: ì§ˆì˜ì‘ë‹µ ìš”ì²­
            
        Returns:
            QueryResponse: ì§ˆì˜ì‘ë‹µ ì‘ë‹µ
        """
        start_time = time.time()
        
        try:
            # ë¬¸ì„œ ê²€ìƒ‰ í•„ìš”: RAG ì‹¤í–‰
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ í•„ìš”: '{request.query}' -> RAG ì‹¤í–‰")
            
            # LangChain ì²´ì¸ ì‹¤í–‰ (ë™ì  thresholdëŠ” ìë™ ê³„ì‚°)
            result = self.rag_chain.invoke({
                "query": request.query,
                "top_k": request.top_k or self.config.RAG_TOP_K
            })
            
            answer = result["answer"]
            retrieved_chunks = result["retrieved_chunks"]
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ: Small talk ì‚¬ìš©í•˜ì§€ ì•Šê³  "ì •ë³´ ì—†ìŒ" ë©”ì‹œì§€
            if not retrieved_chunks:
                logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: '{request.query}' -> ì •ë³´ ë¶€ì¡± ë©”ì‹œì§€ ë°˜í™˜")
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # LangSmithì— ë©”íƒ€ë°ì´í„° ì „ë‹¬ì„ ìœ„í•´ dictë¡œ ë³€í™˜
            response = QueryResponse(
                query=request.query,
                answer=answer,
                retrieved_chunks=retrieved_chunks,
                processing_time=processing_time,
                model_used=self.config.OPENAI_MODEL
            )
            
            # ì‹¤ì‹œê°„ í‰ê°€ ìˆ˜í–‰ (í„°ë¯¸ë„ ì¶œë ¥ìš©)
            # ì‹¤ì‹œê°„ í‰ê°€ ìˆ˜í–‰ (í„°ë¯¸ë„ ì¶œë ¥ìš©) - ë¹„í™œì„±í™” (ì†ë„ ê°œì„  ë° í† í° ì ˆì•½)
            # try:
            #     print("\n" + "="*50)
            #     print("ğŸ” ì‹¤ì‹œê°„ RAG ë‹µë³€ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
            #     # Ground Truth ì¡°íšŒ (í‰ê°€ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©)
            #     ground_truth = self.evaluator.lookup_ground_truth(request.query)
                
            #     eval_result = self.evaluator.evaluate_single(
            #         question=request.query,
            #         answer=answer,
            #         context="\n".join([chunk.text for chunk in retrieved_chunks]),
            #         ground_truth=ground_truth
            #     )
            #     print(f"  - ì •í™•ì„± (Faithfulness): {eval_result.get('faithfulness_score')}ì ")
            #     print(f"  - ì™„ì „ì„± (Completeness): {eval_result.get('completeness_score')}ì ")
            #     print(f"  - ì—°ê´€ì„± (Answer Relevancy): {eval_result.get('answer_relevancy_score')}ì ")
            #     print(f"  - ì •ë°€ë„ (Context Precision): {eval_result.get('context_precision_score')}ì ")
            #     print(f"  - ì¼ì¹˜ë„ (Answer Correctness): {eval_result.get('answer_correctness_score')}ì ")
                
            #     # ê²°ê³¼ JSON ì €ì¥
            #     try:
            #         timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    
            #         # ì ˆëŒ€ ê²½ë¡œ ê³„ì‚°: backend/data/HR_RAG/HR_RAG_result
            #         current_dir = os.path.dirname(os.path.abspath(__file__))
            #         # backend/app/domain/rag/HR -> ... -> backend
            #         backend_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_dir))))
            #         result_dir = os.path.join(backend_dir, "data", "HR_RAG", "HR_RAG_result")
                    
            #         os.makedirs(result_dir, exist_ok=True)
                    
            #         result_file = os.path.join(result_dir, f"evaluation_{timestamp}.json")
                    
            #         # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
            #         save_data = {
            #             "timestamp": timestamp,
            #             "query": request.query,
            #             "answer": answer,
            #             "retrieved_chunks": [
            #                 {
            #                     "filename": chunk.metadata.get("filename", "Unknown"),
            #                     "page": chunk.metadata.get("page_number", "?"),
            #                     "score": chunk.score,
            #                     "text": chunk.text
            #                 } for chunk in retrieved_chunks
            #             ],
            #             "ground_truth": ground_truth,
            #             "evaluation": eval_result
            #         }
                    
            #         with open(result_file, 'w', encoding='utf-8') as f:
            #             json.dump(save_data, f, ensure_ascii=False, indent=4)
                        
            #         logger.info(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_file}")
            #         print(f"  - ê²°ê³¼ íŒŒì¼ ì €ì¥: {result_file}")
                    
            #     except Exception as save_e:
            #         logger.error(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")
            #         print(f"  - ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")

            #     print("="*50 + "\n")
                    
            # except Exception as eval_e:
            #     logger.warning(f"ì‹¤ì‹œê°„ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {eval_e}")
            #     print(f"âŒ ì‹¤ì‹œê°„ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {eval_e}")
            #     print("="*50 + "\n")
            
            # LangSmith ë©”íƒ€ë°ì´í„° ë¡œê¹…
            from langsmith import traceable
            from langsmith.run_helpers import get_current_run_tree
            
            try:
                run_tree = get_current_run_tree()
                if run_tree:
                    run_tree.extra = {
                        "retrieved_chunks_count": len(retrieved_chunks),
                        "chunks": [
                            {
                                "filename": chunk.metadata.get("filename", "Unknown"),
                                "page_number": chunk.metadata.get("page_number", 0),
                                "score": chunk.score
                            }
                            for chunk in retrieved_chunks
                        ],
                        "processing_time": processing_time,
                        "model": self.config.OPENAI_MODEL
                    }
            except Exception as e:
                logger.warning(f"LangSmith ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {e}")
            
            return response
            
        except Exception as e:
            logger.exception("ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜")
            processing_time = time.time() - start_time
            
            return QueryResponse(
                query=request.query,
                answer=f"ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                retrieved_chunks=[],
                processing_time=processing_time,
                model_used=self.config.OPENAI_MODEL
            )
    
