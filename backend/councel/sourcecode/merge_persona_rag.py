"""
í†µí•© RAG + Persona ìƒë‹´ ì‹œìŠ¤í…œ
ìƒì„±ë‚ ì§œ: 2025.11.18
ì„¤ëª…: RAG ê¸°ë°˜ ê²€ìƒ‰ê³¼ Persona(ì¹¼ ë¡œì €ìŠ¤) ìŠ¤íƒ€ì¼ì„ í†µí•©í•œ ìƒë‹´ ì‹œìŠ¤í…œ
      ì‚¬ìš©ì ì…ë ¥ ìœ í˜•ì„ ìë™ íŒë‹¨í•˜ì—¬ ì§ˆë¬¸/ìƒë‹´ì— ë§ëŠ” ë‹µë³€ ì œê³µ
"""

import json
import chromadb
from pathlib import Path
from typing import List, Dict, Any
import torch
from transformers import AutoModel, AutoTokenizer
import re


# ì¹¼ ë¡œì €ìŠ¤ ìƒë‹´ ìŠ¤íƒ€ì¼ Persona ì§€ì¹¨
PERSONA_INSTRUCTIONS = """
ë‹¹ì‹ ì€ ì¹¼ ë¡œì €ìŠ¤(Carl Rogers)ì˜ ì¸ê°„ì¤‘ì‹¬ ìƒë‹´ ì´ë¡ ì„ ë”°ë¥´ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

í•µì‹¬ ì›ì¹™:
1. ë¬´ì¡°ê±´ì  ê¸ì •ì  ì¡´ì¤‘ (Unconditional Positive Regard): ì‚¬ìš©ìë¥¼ ìˆëŠ” ê·¸ëŒ€ë¡œ ìˆ˜ìš©í•˜ê³  ì¡´ì¤‘í•©ë‹ˆë‹¤.
2. ê³µê°ì  ì´í•´ (Empathic Understanding): ì‚¬ìš©ìì˜ ê°ì •ê³¼ ê²½í—˜ì„ ê¹Šì´ ì´í•´í•˜ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤.
3. ì§„ì†”ì„± (Genuineness): ì§„ì‹¤ë˜ê³  ì¼ì¹˜ëœ íƒœë„ë¡œ ëŒ€í•©ë‹ˆë‹¤.

ìƒë‹´ ë°©ì‹:
- Reflective Listening: ì‚¬ìš©ìì˜ ë§ì„ ë°˜ì˜í•˜ê³  ì¬ì§„ìˆ í•©ë‹ˆë‹¤
- ê°ì • ê³µê°: ì‚¬ìš©ìì˜ ê°ì •ì„ ë¨¼ì € ì¸ì‹í•˜ê³  ê³µê°í•©ë‹ˆë‹¤
- ë¹„ì§€ì‹œì  ì ‘ê·¼: ì¡°ì–¸ë³´ë‹¤ëŠ” ì‚¬ìš©ì ìŠ¤ìŠ¤ë¡œ ë‹µì„ ì°¾ë„ë¡ ë•ìŠµë‹ˆë‹¤
- ë”°ëœ»í•˜ê³  ìˆ˜ìš©ì ì¸ í†¤: íŒë‹¨í•˜ì§€ ì•Šê³  ë”°ëœ»í•˜ê²Œ ëŒ€í•©ë‹ˆë‹¤

ë§íˆ¬:
- "~í•˜ì‹œëŠ”êµ°ìš”", "~í•˜ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”" ë“± ë¶€ë“œëŸ½ê³  ì¡´ì¤‘í•˜ëŠ” í‘œí˜„ ì‚¬ìš©
- "ì œê°€ ì´í•´í•œ ë°”ë¡œëŠ”..." ë“±ìœ¼ë¡œ reflective listening í‘œí˜„
- ì§§ê³  ê°„ê²°í•˜ê²Œ, ê·¸ëŸ¬ë‚˜ ë”°ëœ»í•˜ê²Œ
"""


# í†µí•© RAG + Persona ìƒë‹´ ì‹œìŠ¤í…œ
class MergedTherapySystem:
    
    def __init__(self, vector_db_path: str, model_name: str = "BAAI/bge-m3"):
        """
        í†µí•© ìƒë‹´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        - RAG ì—”ì§„: Vector DB ê²€ìƒ‰ ë° ì„ë² ë”©
        - Persona: ì¹¼ ë¡œì €ìŠ¤ ìŠ¤íƒ€ì¼ ìƒë‹´
        """
        # Vector DB ê²½ë¡œ ì„¤ì •
        self.db_path = Path(vector_db_path)
        
        # Vector DB ì¡´ì¬ í™•ì¸
        if not self.db_path.exists():
            raise FileNotFoundError(f"Vector DB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.db_path}")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path=str(self.db_path))
        
        # ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
        self.collection_map = {
            1: "paragraph_vec",  # ë¬¸ë‹¨ ê¸°ë°˜
            2: "semantic_vec"    # ì˜ë¯¸ ê¸°ë°˜
        }
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU/CPU)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        print(f"âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # í˜„ì¬ ì„ íƒëœ ì»¬ë ‰ì…˜ ë° DB ë²ˆí˜¸
        self.current_collection = None
        self.current_db_choice = None
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.chat_history = []
        
        # Persona ì§€ì¹¨
        self.persona_instructions = PERSONA_INSTRUCTIONS
    
    # ========== RAG ì—”ì§„ ê´€ë ¨ í•¨ìˆ˜ ==========
    
    def mean_pooling(self, model_output, attention_mask):
        """í‰ê·  í’€ë§ í•¨ìˆ˜"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def create_query_embedding(self, query_text: str) -> List[float]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        # í† í°í™”
        encoded_input = self.tokenizer(
            query_text,
            padding=True,
            truncation=True,
            return_tensors='pt',
            max_length=512
        )
        
        # GPU/CPUë¡œ ì´ë™
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            model_output = self.model(**encoded_input)
            embedding = self.mean_pooling(model_output, encoded_input['attention_mask'])
            embedding = embedding.cpu().numpy()[0]
        
        return embedding.tolist()
    
    def select_collection(self, db_choice: int) -> bool:
        """Vector DB ì»¬ë ‰ì…˜ ì„ íƒ (Rule 1)"""
        # ì˜ëª»ëœ ê°’ ì…ë ¥ ì‹œ ì˜ˆì™¸ì²˜ë¦¬
        if db_choice not in self.collection_map:
            print(f"[ì˜¤ë¥˜] ì˜ëª»ëœ DB ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return False
        
        # ì„ íƒí•œ DB ì´ë¦„ì„ ë³€ìˆ˜ì— ì €ì¥
        collection_name = self.collection_map[db_choice]
        
        try:
            self.current_collection = self.client.get_collection(name=collection_name)
            self.current_db_choice = db_choice
            collection_type = "ë¬¸ë‹¨ ê¸°ë°˜" if db_choice == 1 else "ì˜ë¯¸ ê¸°ë°˜"
            print(f"\nâœ“ '{collection_type}' Vector DB ì„ íƒ ì™„ë£Œ (ì»¬ë ‰ì…˜: {collection_name})")
            return True
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {collection_name}")
            print(f"ìƒì„¸: {e}")
            return False
    
    def retrieve_chunks(self, user_input: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ìƒë‹´ ì²­í¬ë¡œë¶€í„° ê²€ìƒ‰ (Rule 1)"""
        if self.current_collection is None:
            print("[ì˜¤ë¥˜] Vector DBê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = self.create_query_embedding(user_input)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = self.current_collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        retrieved_chunks = []
        if results['ids'] and results['ids'][0]:
            for i in range(len(results['ids'][0])):
                chunk = {
                    'id': results['ids'][0][i],
                    'text': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i] if 'distances' in results else None
                }
                retrieved_chunks.append(chunk)
        
        return retrieved_chunks
    
    def generate_rag_answer(self, user_input: str, retrieved_chunks: List[Dict[str, Any]]) -> str:
        """RAG ì—”ì§„ì—ì„œ ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ë‹µë³€ ìƒì„±"""
        if not retrieved_chunks:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ìƒìœ„ 3ê°œ ì²­í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ êµ¬ì„±
        rag_answer_parts = []
        for i, chunk in enumerate(retrieved_chunks[:3], 1):
            chunk_text = chunk['text']
            rag_answer_parts.append(chunk_text)
        
        return "\n\n".join(rag_answer_parts)
    
    # ========== Persona ê´€ë ¨ í•¨ìˆ˜ ==========
    
    def classify_input_type_auto(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì´ ì§ˆë¬¸ì¸ì§€ ìƒë‹´ì¸ì§€ ìë™ìœ¼ë¡œ íŒë‹¨ (Rule 2)
        ê°ì • í‘œí˜„, ê³ ë¯¼, ì‹¬ë¦¬ì  ì–´ë ¤ì›€ì´ í¬í•¨ë˜ë©´ 'counseling'
        ì •ë³´ ìš”ì²­, ì‚¬ì‹¤ í™•ì¸ ë“±ì€ 'question'
        """
        # ê°ì •/ê³ ë¯¼ í‚¤ì›Œë“œ (ìƒë‹´ íŒë‹¨ìš©)
        counseling_patterns = [
            r'í˜ë“¤', r'ì–´ë µ', r'ê´´ë¡­', r'ìš°ìš¸', r'ë¶ˆì•ˆ', r'ìŠ¬í”„', r'í™”ê°€', r'ì™¸ë¡œ', r'ë¬´ì„­',
            r'ê³ ë¯¼', r'ê±±ì •', r'ë‘ë µ', r'ë‹µë‹µ', r'ì†ìƒ', r'ì§œì¦', r'ìŠ¤íŠ¸ë ˆìŠ¤', r'í”¼ê³¤',
            r'ì§€ì³', r'ì¢Œì ˆ', r'ì‹¤ë§', r'í›„íšŒ', r'ë¯¸ì•ˆ', r'ì£„ì±…ê°',
            r'ì–´ë–»ê²Œ í•´ì•¼', r'ì¡°ì–¸', r'ë„ì›€', r'ìƒë‹´', r'ì´ì•¼ê¸° ë“¤ì–´',
            r'ë‚˜ëŠ”.*ëŠë‚€ë‹¤', r'ë‚˜ëŠ”.*ìƒê°í•œë‹¤', r'ë‚´ê°€.*í•˜ëŠ”ë°'
        ]
        
        # ì§ˆë¬¸ íŒ¨í„´ (ì§ˆë¬¸ íŒë‹¨ìš©)
        question_patterns = [
            r'ë¬´ì—‡', r'ë¬´ìŠ¨', r'ì–´ë–¤', r'ì–¸ì œ', r'ì–´ë””', r'ëˆ„ê°€', r'ì™œ', r'ì–´ë–»ê²Œ',
            r'ì´ë€', r'ë€', r'ì´ë‹¤', r'ì¸ê°€', r'ì…ë‹ˆê¹Œ',
            r'\?', r'ì•Œë ¤', r'ì„¤ëª…', r'ì •ì˜', r'ì˜ë¯¸', r'ì°¨ì´', r'ë°©ë²•'
        ]
        
        # ìƒë‹´ íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜
        counseling_score = sum(1 for pattern in counseling_patterns if re.search(pattern, user_input))
        
        # ì§ˆë¬¸ íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜
        question_score = sum(1 for pattern in question_patterns if re.search(pattern, user_input))
        
        # ì ìˆ˜ ê¸°ë°˜ íŒë‹¨
        if counseling_score > question_score:
            return "counseling"
        elif question_score > counseling_score:
            return "question"
        else:
            # ë™ì ì´ê±°ë‚˜ ë‘˜ ë‹¤ 0ì¸ ê²½ìš°, ë¬¸ì¥ ê¸¸ì´ì™€ êµ¬ì¡°ë¡œ íŒë‹¨
            if len(user_input) > 30 and any(char in user_input for char in ['..', '...']):
                return "counseling"
            elif '?' in user_input or len(user_input) < 30:
                return "question"
            else:
                # ê¸°ë³¸ê°’ì€ ìƒë‹´ìœ¼ë¡œ ê°„ì£¼ (ì•ˆì „í•œ ì„ íƒ)
                return "counseling"
    
    def _extract_emotion(self, user_input: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ê°ì •ì„ ì¶”ì¶œí•˜ì—¬ reflective listeningì— í™œìš© (Rule 3)"""
        emotion_keywords = {
            'í˜ë“¤': 'í˜ë“œì‹œê²Œ',
            'ì–´ë ¤': 'ì–´ë ¤ìš°ì‹œê²Œ',
            'ê´´ë¡­': 'ê´´ë¡œìš°ì‹œê²Œ',
            'ìš°ìš¸': 'ìš°ìš¸í•˜ì‹œê²Œ',
            'ë¶ˆì•ˆ': 'ë¶ˆì•ˆí•˜ì‹œê²Œ',
            'ìŠ¬í”„': 'ìŠ¬í”„ì‹œê²Œ',
            'í™”ê°€': 'í™”ê°€ ë‚˜ì‹œê²Œ',
            'ì™¸ë¡œ': 'ì™¸ë¡œìš°ì‹œê²Œ',
            'ê±±ì •': 'ê±±ì •ë˜ì‹œê²Œ',
            'ë‘ë µ': 'ë‘ë ¤ìš°ì‹œê²Œ',
            'ë‹µë‹µ': 'ë‹µë‹µí•˜ì‹œê²Œ',
            'ì†ìƒ': 'ì†ìƒí•˜ì‹œê²Œ',
            'ìŠ¤íŠ¸ë ˆìŠ¤': 'ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ì‹œê²Œ',
            'ì§€ì³': 'ì§€ì¹˜ì‹œê²Œ',
        }
        
        for keyword, emotion in emotion_keywords.items():
            if keyword in user_input:
                return emotion
        
        # ê¸°ë³¸ ê°ì • í‘œí˜„
        return "ì—¬ëŸ¬ ê°ì •ì„"
    
    def _add_history_context(self, answer: str) -> str:
        """ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì— ë§¥ë½ì„ ì¶”ê°€ (Rule 4)"""
        # ì´ì „ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ì—°ê²° í‘œí˜„ ì¶”ê°€
        if len(self.chat_history) > 0:
            last_exchange = self.chat_history[-1]
            # í•„ìš”ì‹œ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ ìˆ˜ì •
            # í˜„ì¬ëŠ” ë‹¨ìˆœíˆ ì—°ê²°ì„± ìœ ì§€
            pass
        
        return answer
    
    def _apply_persona_layer(self, chunk_text: str) -> str:
        """
        RAG ì²­í¬ë¥¼ ì¹¼ ë¡œì €ìŠ¤ ìƒë‹´ì‚¬ ì‹œì ìœ¼ë¡œ ì¬ì‘ì„± (Persona Layer)
        - ì´ë¡ ì  ë‚´ìš©ì„ ìƒë‹´ì‚¬ì˜ ë¶€ë“œëŸ¬ìš´ ë§íˆ¬ë¡œ ë³€í™˜
        - "~ì…ë‹ˆë‹¤" â†’ "~ì¸ ê²ƒ ê°™ì•„ìš”", "~í•˜ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”"
        - ê°ê´€ì  ì„¤ëª… â†’ ê³µê°ì  ì œì•ˆ
        """
        # ì²­í¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not chunk_text.strip():
            return chunk_text
        
        # ìƒë‹´ì‚¬ ì‹œì  ë³€í™˜ íŒ¨í„´
        # 1. ê°•í•œ ë‹¨ì • í‘œí˜„ì„ ë¶€ë“œëŸ½ê²Œ
        text = re.sub(r'ì…ë‹ˆë‹¤\.', 'ì¸ ê²ƒ ê°™ì•„ìš”.', chunk_text)
        text = re.sub(r'í•©ë‹ˆë‹¤\.', 'í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”.', text)
        text = re.sub(r'ë©ë‹ˆë‹¤\.', 'ë  ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”.', text)
        text = re.sub(r'ìˆìŠµë‹ˆë‹¤\.', 'ìˆëŠ” ê²ƒ ê°™ì•„ìš”.', text)
        
        # 2. "í•´ì•¼ í•œë‹¤" ê°™ì€ ì§€ì‹œì  í‘œí˜„ì„ ë¹„ì§€ì‹œì ìœ¼ë¡œ
        text = re.sub(r'í•´ì•¼ í•©ë‹ˆë‹¤', 'í•´ë³´ì‹œëŠ” ê²ƒë„ ë„ì›€ì´ ë  ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”', text)
        text = re.sub(r'í•´ì•¼ í•œë‹¤', 'í•´ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”', text)
        text = re.sub(r'í•„ìš”í•©ë‹ˆë‹¤', 'í•„ìš”í•˜ì‹¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”', text)
        
        # 3. "ì¤‘ìš”í•˜ë‹¤" ê°™ì€ ê°•ì¡° í‘œí˜„ì„ ë¶€ë“œëŸ½ê²Œ
        text = re.sub(r'ì¤‘ìš”í•©ë‹ˆë‹¤', 'ì¤‘ìš”í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”', text)
        text = re.sub(r'ì¤‘ìš”í•˜ë‹¤', 'ì¤‘ìš”í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”', text)
        
        # 4. ì´ë¡ ì  ìš©ì–´ë¥¼ ì¼ìƒì  í‘œí˜„ìœ¼ë¡œ
        text = re.sub(r'ê³µê°ì  ì´í•´', 'ìƒëŒ€ë°©ì˜ ë§ˆìŒì„ ì´í•´í•˜ë ¤ëŠ” ë…¸ë ¥', text)
        text = re.sub(r'ë¬´ì¡°ê±´ì  ê¸ì •ì  ì¡´ì¤‘', 'ìˆëŠ” ê·¸ëŒ€ë¡œë¥¼ ë°›ì•„ë“¤ì´ëŠ” ê²ƒ', text)
        
        return text
    
    def _transform_chunk_to_counseling(self, chunk_text: str) -> str:
        """
        RAG ì²­í¬ë¥¼ ìƒë‹´ì‚¬ ê´€ì ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì¬êµ¬ì„±
        - ë‹¨ìˆœ ì •ë³´ ë‚˜ì—´ì´ ì•„ë‹Œ, ìƒë‹´ì‚¬ê°€ ì´í•´í•˜ê³  ê³µê°í•˜ë©° ì „ë‹¬í•˜ëŠ” í˜•ì‹
        """
        # Persona Layer ì ìš©
        transformed = self._apply_persona_layer(chunk_text)
        
        # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½í•˜ê±°ë‚˜ í•µì‹¬ë§Œ ì¶”ì¶œ
        sentences = transformed.split('.')
        
        # í•µì‹¬ ë¬¸ì¥ë§Œ ì„ íƒ (ë„ˆë¬´ ê¸°ìˆ ì ì´ê±°ë‚˜ í˜•ì‹ì ì¸ ë¬¸ì¥ ì œì™¸)
        meaningful_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 10:
                # ë„ˆë¬´ í•™ìˆ ì ì´ê±°ë‚˜ í˜•ì‹ì ì¸ í‘œí˜„ í•„í„°ë§
                if not any(word in sentence for word in ['ì°¸ê³ ë¬¸í—Œ', 'ì¶œì²˜:', 'ì €ì:', 'ë…¼ë¬¸', 'ì—°êµ¬']):
                    meaningful_sentences.append(sentence)
        
        # ìµœëŒ€ 2-3ê°œ ë¬¸ì¥ë§Œ ì‚¬ìš© (ê°„ê²°í•˜ê²Œ)
        if len(meaningful_sentences) > 3:
            meaningful_sentences = meaningful_sentences[:3]
        
        # ì¬ì¡°í•©
        result = '. '.join(meaningful_sentences)
        if result and not result.endswith('.'):
            result += '.'
        
        return result
    
    # ========== í†µí•© ë‹µë³€ ìƒì„± í•¨ìˆ˜ ==========
    
    def generate_final_answer(
        self,
        user_input: str,
        input_type: str,
        rag_answer: str,
        retrieved_chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ì…ë ¥ íƒ€ì…ì— ë”°ë¼ ìµœì¢… ë‹µë³€ ìƒì„± (Rule 2)
        - question: RAG ë‹µë³€ì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€
        - counseling: RAG ë‹µë³€ì„ Persona ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•˜ì—¬ ê³µê°ì  ìƒë‹´ ì§„í–‰
        """
        
        # Rule 2: ì§ˆë¬¸ì¸ ê²½ìš° - ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€ (Rule 5, 6)
        if input_type == "question":
            if not retrieved_chunks:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
            else:
                # RAG ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ ì œê³µ
                answer_parts = []
                answer_parts.append(f"ì§ˆë¬¸í•´ì£¼ì‹  '{user_input}'ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n")
                answer_parts.append(rag_answer)
                answer_parts.append("\n\në” ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.")
                answer = "".join(answer_parts)
        
        # Rule 2: ìƒë‹´ì¸ ê²½ìš° - Persona ìŠ¤íƒ€ì¼ ì ìš© (Rule 3: ê³µê°, reflective listening)
        else:  # counseling
            if not retrieved_chunks:
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ê³µê°ì ìœ¼ë¡œ ì‘ë‹µ
                answer = f"ë§ì”€í•´ì£¼ì‹  '{user_input}'ì— ëŒ€í•´ í•¨ê»˜ ìƒê°í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ëŠë¼ì‹œëŠ” ê°ì •ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ë“¤ë ¤ì£¼ì‹œê² ì–´ìš”?"
            else:
                # Persona ìŠ¤íƒ€ì¼ ì ìš©: ì¹¼ ë¡œì €ìŠ¤ ë°©ì‹ì˜ ê³µê°ì  ìƒë‹´
                answer_parts = []
                
                # 1. Reflective Listening: ì‚¬ìš©ì ë§ ë°˜ì˜
                answer_parts.append(f"ë§ì”€í•˜ì‹  ê²ƒì„ ë“¤ìœ¼ë‹ˆ, {self._extract_emotion(user_input)} ëŠë¼ê³  ê³„ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”.")
                
                # 2. ê³µê° í‘œí˜„
                answer_parts.append(" ê·¸ëŸ° ìƒí™©ì´ë¼ë©´ ì¶©ë¶„íˆ ê·¸ë ‡ê²Œ ëŠë¼ì‹¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
                
                # 3. RAG ê¸°ë°˜ ìƒë‹´ ë‚´ìš© ì œê³µ (Persona Layerë¡œ ì¬ì‘ì„±)
                answer_parts.append("\n\nì œê°€ ì´í•´í•œ ë°”ë¡œëŠ”, ì´ëŸ° ê´€ì ë„ ë„ì›€ì´ ë  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤:\n")
                
                # ì²­í¬ ë‚´ìš©ì„ ìƒë‹´ì‚¬ ì‹œì ìœ¼ë¡œ ì¬ì‘ì„± (Persona Layer ì ìš©)
                counseling_chunks = []
                for i, chunk in enumerate(retrieved_chunks[:2], 1):  # ìƒìœ„ 2ê°œë§Œ ì‚¬ìš© (ê°„ê²°í•˜ê²Œ)
                    chunk_text = chunk['text']
                    # â˜… Persona Layer: ì²­í¬ë¥¼ ìƒë‹´ì‚¬ ì‹œì ìœ¼ë¡œ ë³€í™˜
                    transformed_text = self._transform_chunk_to_counseling(chunk_text)
                    
                    if transformed_text.strip():
                        counseling_chunks.append(transformed_text)
                
                # ì¬ì‘ì„±ëœ ì²­í¬ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
                if counseling_chunks:
                    # ì²« ë²ˆì§¸ ì²­í¬
                    answer_parts.append(f"\n{counseling_chunks[0]}")
                    
                    # ë‘ ë²ˆì§¸ ì²­í¬ê°€ ìˆìœ¼ë©´ ì—°ê²°ì–´ì™€ í•¨ê»˜ ì¶”ê°€
                    if len(counseling_chunks) > 1:
                        answer_parts.append(f"\n\në˜í•œ, {counseling_chunks[1]}")
                
                # 4. ë§ˆë¬´ë¦¬: ë¹„ì§€ì‹œì  ì ‘ê·¼ (ì¡°ì–¸ë³´ë‹¤ëŠ” íƒìƒ‰ ìœ ë„)
                answer_parts.append("\n\nì´ëŸ° ì´ì•¼ê¸°ë“¤ì´ ì§€ê¸ˆì˜ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ëŠê»´ì§€ì‹œë‚˜ìš”? ë” ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹  ì´ì•¼ê¸°ê°€ ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.")
                
                answer = "".join(answer_parts)
        
        # Rule 4: chat_historyë¥¼ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        if self.chat_history:
            answer = self._add_history_context(answer)
        
        return {
            "input_type": input_type,
            "answer": answer,
            "used_rag_answer": rag_answer[:100] + "..." if len(rag_answer) > 100 else rag_answer,
            "db_used": self.current_db_choice,
            "continue_conversation": True
        }
    
    # ========== ë©”ì¸ ìƒë‹´ í•¨ìˆ˜ ==========
    
    def chat(self, user_input: str) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì „ì²´ ìƒë‹´ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
        1. ì…ë ¥ íƒ€ì… ìë™ íŒë‹¨ (question / counseling)
        2. ì„ íƒëœ Vector DBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        3. RAG ê¸°ë³¸ ë‹µë³€ ìƒì„±
        4. ì…ë ¥ íƒ€ì…ì— ë”°ë¼ ìµœì¢… ë‹µë³€ ìƒì„± (Persona ì ìš©)
        5. ëŒ€í™” ê¸°ë¡ ì €ì¥ ë° ì—°ì†ì„± ìœ ì§€
        """
        
        # Rule 7: exit ì…ë ¥ í™•ì¸
        if user_input.strip().lower() == "exit":
            return {
                "input_type": "exit",
                "answer": "ìƒë‹´ì„ ë§ˆë¬´ë¦¬í•˜ê² ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì–¸ì œë“  ë‹¤ì‹œ ì°¾ì•„ì£¼ì„¸ìš”.",
                "used_rag_answer": "",
                "db_used": self.current_db_choice,
                "continue_conversation": False
            }
        
        # Step 1: ì…ë ¥ íƒ€ì… ìë™ íŒë‹¨ (Rule 2)
        input_type = self.classify_input_type_auto(user_input)
        print(f"\nğŸ” ì…ë ¥ ë¶„ì„: {'ìƒë‹´' if input_type == 'counseling' else 'ì§ˆë¬¸'} ëª¨ë“œ")
        
        # Step 2: ì„ íƒëœ Vector DBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (Rule 1)
        print("ğŸ” ê´€ë ¨ ìë£Œ ê²€ìƒ‰ ì¤‘...")
        retrieved_chunks = self.retrieve_chunks(user_input, n_results=5)
        print(f"âœ“ {len(retrieved_chunks)}ê°œì˜ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
        
        # Step 3: RAG ê¸°ë³¸ ë‹µë³€ ìƒì„±
        rag_answer = self.generate_rag_answer(user_input, retrieved_chunks)
        
        # Step 4: ìµœì¢… ë‹µë³€ ìƒì„± (ì…ë ¥ íƒ€ì…ì— ë”°ë¼ Persona ì ìš©)
        response = self.generate_final_answer(
            user_input=user_input,
            input_type=input_type,
            rag_answer=rag_answer,
            retrieved_chunks=retrieved_chunks
        )
        
        # Step 5: ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€ (Rule 4: ì—°ì†ì„± ìœ ì§€)
        self.chat_history.append({
            "user": user_input,
            "assistant": response["answer"],
            "type": input_type,
            "db_used": self.current_db_choice
        })
        
        return response


# ========== ë©”ì¸ í•¨ìˆ˜ ==========

def main():
    """
    í†µí•© ìƒë‹´ ì‹œìŠ¤í…œ ë©”ì¸ í•¨ìˆ˜
    - DB ì„ íƒ ì•ˆë‚´ (Rule 1)
    - ë°˜ë³µ ëŒ€í™” ìœ ì§€ (Rule 7)
    """
    
    print("=" * 70)
    print("í†µí•© RAG + Persona ìƒë‹´ ì‹œìŠ¤í…œ (ì¹¼ ë¡œì €ìŠ¤ ìŠ¤íƒ€ì¼)")
    print("=" * 70)
    
    # ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).parent.parent
    vector_db_dir = base_dir / "vector_db"
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\nì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n")
        therapy_system = MergedTherapySystem(str(vector_db_dir))
        
        # Rule 1: DB ì„ íƒ ì•ˆë‚´
        print("\n" + "=" * 70)
        print("Vector DB ì„ íƒ")
        print("=" * 70)
        print("1. ë¬¸ë‹¨ ê¸°ë°˜ (paragraph_vec)")
        print("2. ì˜ë¯¸ ê¸°ë°˜ (semantic_vec)")
        print("=" * 70)
        
        while True:
            db_choice = input("\nDB ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()
            
            if db_choice in ['1', '2']:
                if therapy_system.select_collection(int(db_choice)):
                    break
            else:
                print("[ì˜¤ë¥˜] 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ìƒë‹´ ì‹œì‘
        print("\n" + "=" * 70)
        print("ìƒë‹´ ì‹œì‘")
        print("=" * 70)
        print("ì§ˆë¬¸ì´ë‚˜ ê³ ë¯¼ì„ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.")
        print("- ì¼ë°˜ ì§ˆë¬¸: ì •ë³´ ê¸°ë°˜ ë‹µë³€ ì œê³µ")
        print("- ìƒë‹´/ê³ ë¯¼: ì¹¼ ë¡œì €ìŠ¤ ìŠ¤íƒ€ì¼ ê³µê°ì  ìƒë‹´ ì§„í–‰")
        print("ì¢…ë£Œí•˜ì‹œë ¤ë©´ 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 70)
        
        # Rule 7: ëŒ€í™” ë£¨í”„ (ë°˜ë³µ ëŒ€í™” ìœ ì§€)
        while True:
            print("\n" + "-" * 70)
            user_input = input("\n[ì‚¬ìš©ì] ").strip()
            
            if not user_input:
                print("[ì•Œë¦¼] ì§ˆë¬¸ì´ë‚˜ ê³ ë¯¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ìƒë‹´ ì§„í–‰
            response = therapy_system.chat(user_input)
            
            # ë‹µë³€ ì¶œë ¥
            print("\n[ìƒë‹´ì‚¬]")
            print(response['answer'])
            
            # ë””ë²„ê¹… ì •ë³´ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # print(f"\n[DEBUG] ì…ë ¥ íƒ€ì…: {response['input_type']}")
            # print(f"[DEBUG] ì‚¬ìš©ëœ DB: {response['db_used']}")
            # print(f"[DEBUG] RAG ë‹µë³€ ìš”ì•½: {response['used_rag_answer']}")
            
            # ì¢…ë£Œ í™•ì¸
            if not response['continue_conversation']:
                break
        
        print("\n" + "=" * 70)
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("=" * 70)
    
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

