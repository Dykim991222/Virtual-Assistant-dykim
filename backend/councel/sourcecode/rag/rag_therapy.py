"""
RAG ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ
ìƒì„±ë‚ ì§œ: 2025.11.18
ì„¤ëª…: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ ê´€ë ¨ ìƒë‹´ ë°ì´í„° ì²­í¬ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë‹µë³€ ë˜ëŠ” ìƒë‹´ì„ ì§„í–‰
"""

import json
import chromadb
from pathlib import Path
from typing import List, Dict, Any
import torch
from transformers import AutoModel, AutoTokenizer


# RAG ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ
class RAGTherapySystem:
    
    # ì´ˆê¸°í™” í•¨ìˆ˜
    def __init__(self, vector_db_path: str, model_name: str = "BAAI/bge-m3"):

        # Vector DB ê²½ë¡œ ì„¤ì •
        self.db_path = Path(vector_db_path)
        
        # Vector DB ì¡´ì¬ í™•ì¸
        if not self.db_path.exists():
            raise FileNotFoundError(f"Vector DB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.db_path}")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path=str(self.db_path))
        
        # ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
        self.collection_map = {
            1: "paragraph_vec",  # ë¬¸ë‹¨ ê¸°ë°˜
            2: "semantic_vec"    # ì˜ë¯¸ ê¸°ë°˜
        }
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU/CPU)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        # í˜„ì¬ ì„ íƒëœ ì»¬ë ‰ì…˜
        self.current_collection = None
    
    # í‰ê·  í’€ë§ í•¨ìˆ˜ - ìì„¸í•œê±´ automatic_save/create_embeddings.py 30~36ì¤„ ì°¸ê³ 
    def mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    def create_query_embedding(self, query_text: str) -> List[float]:

        # í† í°í™”
        encoded_input = self.tokenizer(
            query_text,
            padding=True,
            truncation=True,
            return_tensors='pt',
            max_length=512
        )                  
        
        # GPU/CPUë¡œ ì´ë™
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            model_output = self.model(**encoded_input)
            embedding = self.mean_pooling(model_output, encoded_input['attention_mask'])
            embedding = embedding.cpu().numpy()[0]
        
        return embedding.tolist()
    
    # ì–´ë–¤ Vector DBë¥¼ ì‚¬ìš©í• ê±´ì§€ ì„ íƒí•˜ëŠ” í•¨ìˆ˜
    def select_collection(self, db_choice: int) -> bool:

        # ë§Œì•½ ì˜ëª»ëœ ê°’ ì…ë ¥ ì‹œ ì˜ˆì™¸ì²˜ë¦¬
        if db_choice not in self.collection_map:
            print(f"[ì˜¤ë¥˜] ì˜ëª»ëœ DB ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return False
        
        # ì„ íƒí•œ DB ì´ë¦„ì„ ë³€ìˆ˜ì— ì €ì¥
        collection_name = self.collection_map[db_choice]
        
        try:
            self.current_collection = self.client.get_collection(name=collection_name)
            collection_type = "ë¬¸ë‹¨ ê¸°ë°˜" if db_choice == 1 else "ì˜ë¯¸ ê¸°ë°˜"
            print(f"\nâœ“ '{collection_type}' Vector DB ì„ íƒ ì™„ë£Œ (ì»¬ë ‰ì…˜: {collection_name})") # console ì¶œë ¥ ìš©ë„, ë‚˜ì¤‘ì— ì‚­ì œ ì˜ˆì •
            return True
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {collection_name}")
            print(f"ìƒì„¸: {e}")
            return False
    
    # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ìƒë‹´ ì²­í¬ë¡œë¶€í„° ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    def retrieve_chunks(self, user_input: str, n_results: int = 5) -> List[Dict[str, Any]]:

        if self.current_collection is None:
            print("[ì˜¤ë¥˜] Vector DBê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = self.create_query_embedding(user_input)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = self.current_collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        retrieved_chunks = []
        if results['ids'] and results['ids'][0]:
            for i in range(len(results['ids'][0])):
                chunk = {
                    'id': results['ids'][0][i],
                    'text': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i] if 'distances' in results else None
                }
                retrieved_chunks.append(chunk)
        
        return retrieved_chunks
    
    # ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒë‹´ ë‹µë³€ ìƒì„±
    def generate_response(self, user_input: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:

        # ê²€ìƒ‰ëœ ì²­í¬ê°€ ì—†ëŠ” ê²½ìš°
        if not retrieved_chunks:
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ìƒë‹´ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?",
                "used_chunks": [],
                "continue_conversation": True
            }
        
        # ì²­í¬ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        # Rule 1: ì‚¬ìš©ìì˜ ê°ì •ì„ ë¨¼ì € ê³µê°í•˜ê³  ë°˜ì˜
        # Rule 2: retrieved_chunks ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ ë‹µë³€ ì‘ì„±
        # Rule 3: RAG ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•œ ì •ë³´ ì œê³µ
        # Rule 4: ë¶ˆí•„ìš”í•œ ì¥í™©í•œ ì„¤ëª…ì´ë‚˜ ê°œì¸ì  ì˜ê²¬ì€ ë°°ì œ
        # Rule 5: ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
        # Rule 6: ëŒ€í™” í†¤ì€ ì¤‘ë¦½ì ì´ê³  ìƒë‹´ì ì´ë©° ì¹œì ˆí•˜ê²Œ ìœ ì§€
        # Rule 10: ì¶œì²˜ í‘œì‹œ
        
        # ë‹µë³€ êµ¬ì„±
        answer_parts = []
        
        # ê³µê° í‘œí˜„
        answer_parts.append("ë§ì”€í•´ì£¼ì‹  ë‚´ìš©ì„ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì²­í¬ ê¸°ë°˜ ë‹µë³€ ì‘ì„±
        answer_parts.append("\n\nê´€ë ¨ ìƒë‹´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤:\n")
        
        used_chunks = []
        for i, chunk in enumerate(retrieved_chunks[:3], 1):  # ìƒìœ„ 3ê°œ ì²­í¬ ì‚¬ìš©
            chunk_text = chunk['text']
            source = chunk['metadata'].get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            
            # ì²­í¬ ë‚´ìš© ì¶”ê°€
            answer_parts.append(f"\n{i}. {chunk_text}")
            answer_parts.append(f"   (ì¶œì²˜: {source})")
            
            # ì‚¬ìš©ëœ ì²­í¬ ìš”ì•½
            used_chunks.append(f"{source}: {chunk_text[:50]}...")
        
        # ë§ˆë¬´ë¦¬ ë©˜íŠ¸
        answer_parts.append("\n\në” ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”.")
        
        answer = "".join(answer_parts)
        
        return {
            "answer": answer,
            "used_chunks": used_chunks,
            "continue_conversation": True
        }
    
    # ìƒë‹´ í•¨ìˆ˜ 
    def chat(self, user_input: str) -> Dict[str, Any]:

        # exit ì…ë ¥ í™•ì¸ (Rule 9)
        if user_input.strip().lower() == "exit":
            return {
                "answer": "ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì–¸ì œë“  ë‹¤ì‹œ ì°¾ì•„ì£¼ì„¸ìš”. ê°ì‚¬í•©ë‹ˆë‹¤.",
                "used_chunks": [],
                "continue_conversation": False
            }
        
        # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        print("\nğŸ” ê´€ë ¨ ìƒë‹´ ìë£Œ ê²€ìƒ‰ ì¤‘...")
        retrieved_chunks = self.retrieve_chunks(user_input, n_results=5)
        print(f"âœ“ {len(retrieved_chunks)}ê°œì˜ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
        
        # 2. ë‹µë³€ ìƒì„±
        response = self.generate_response(user_input, retrieved_chunks)
        
        return response

# ë©”ì¸
def main():
    
    # console ì¶œë ¥ ìš©ë„, ë‚˜ì¤‘ì— ì‚­ì œ ì˜ˆì •
    print("=" * 70)
    print("RAG ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ")
    print("=" * 70)
    
    # ê²½ë¡œ ì„¤ì • (sourcecode/rag ê¸°ì¤€)
    base_dir = Path(__file__).parent.parent.parent
    vector_db_dir = base_dir / "vector_db"
    
    try:
        # RAG ìƒë‹´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\nì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n") # console ì¶œë ¥ ìš©ë„, ë‚˜ì¤‘ì— ì‚­ì œ ì˜ˆì •
        rag_system = RAGTherapySystem(str(vector_db_dir))
        
        # DB ì„ íƒ
        print("\n" + "=" * 70)
        print("Vector DB ì„ íƒ")
        print("=" * 70)
        print("1. ë¬¸ë‹¨ ê¸°ë°˜ (paragraph_vec)")
        print("2. ì˜ë¯¸ ê¸°ë°˜ (semantic_vec)")
        print("=" * 70)
        
        while True:
            db_choice = input("\nDB ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()
            
            if db_choice in ['1', '2']:
                if rag_system.select_collection(int(db_choice)):
                    break
            else:
                print("[ì˜¤ë¥˜] 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ìƒë‹´ ì‹œì‘
        print("\n" + "=" * 70)
        print("ìƒë‹´ ì‹œì‘")
        print("=" * 70)
        print("ê³ ë¯¼ì´ë‚˜ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.")
        print("ì¢…ë£Œí•˜ì‹œë ¤ë©´ 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 70)
        
        # ëŒ€í™” ë£¨í”„
        while True:
            print("\n" + "-" * 70)
            user_input = input("\n[ì‚¬ìš©ì] ").strip()
            
            if not user_input:
                print("[ì•Œë¦¼] ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ìƒë‹´ ì§„í–‰
            response = rag_system.chat(user_input)
            
            # ë‹µë³€ ì¶œë ¥
            print("\n[ìƒë‹´ì‚¬]")
            print(response['answer'])
            
            # ì‚¬ìš©ëœ ì²­í¬ ì •ë³´ (ë””ë²„ê¹…ìš©, í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            if response['used_chunks']:
                print("\n[ì°¸ê³ í•œ ìë£Œ]")
                for i, chunk in enumerate(response['used_chunks'], 1):
                    print(f"  {i}. {chunk}")
            
            # ì¢…ë£Œ í™•ì¸
            if not response['continue_conversation']:
                break
        
        print("\n" + "=" * 70)
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("=" * 70)
    
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

