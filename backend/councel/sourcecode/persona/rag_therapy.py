"""
RAG ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ
ìƒì„±ë‚ ì§œ: 2025.11.18
ìˆ˜ì •ë‚ ì§œ: 2025.11.21
ì„¤ëª…: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ ê´€ë ¨ ìƒë‹´ ë°ì´í„° ì²­í¬ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë‹µë³€ ë˜ëŠ” ìƒë‹´ì„ ì§„í–‰
OpenAI APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ë° ë‹µë³€ ìƒì„±
"""

import os
import json
import chromadb
from pathlib import Path
from typing import List, Dict, Any, Optional
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# RAG ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ
class RAGTherapySystem:
    
    # ì´ˆê¸°í™” í•¨ìˆ˜
    def __init__(self, vector_db_path: str):

        # Vector DB ê²½ë¡œ ì„¤ì •
        self.db_path = Path(vector_db_path)
        
        # Vector DB ì¡´ì¬ í™•ì¸
        if not self.db_path.exists():
            raise FileNotFoundError(f"Vector DB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤") # ë‚˜ì¤‘ì— ì‚­ì œ ì˜ˆì •
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path=str(self.db_path))
        
        # ì»¬ë ‰ì…˜ ì´ë¦„ (save_to_vectordb.pyì™€ ë™ì¼)
        self.collection_name = "vector_adler"
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.openai_client = OpenAI(api_key=api_key)
        
        # ì»¬ë ‰ì…˜ ë¡œë“œ
        try:
            self.collection = self.client.get_collection(name=self.collection_name)
            print(f"ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            raise ValueError(f"ì»¬ë ‰ì…˜ '{self.collection_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        
        # ê°ì •/ìƒë‹´ í‚¤ì›Œë“œ ëª©ë¡
        self.counseling_keywords = [
            "í˜ë“¤ì–´", "ìƒë‹´", "ì§œì¦", "ìš°ìš¸", "ë¶ˆì•ˆ", "ìŠ¤íŠ¸ë ˆìŠ¤", 
            "ê³ ë¯¼", "ê±±ì •", "ìŠ¬í”„", "ì™¸ë¡œ", "í™”ë‚˜", "ë‹µë‹µ",
            "counseling", "therapy", "help", "depressed", "anxious"
        ]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ (ë‹¨ê¸° ê¸°ì–µ)
        self.chat_history = []
        
        # í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ì •ì˜
        self.adler_persona = """ë‹¹ì‹ ì€ ì•Œí”„ë ˆë“œ ì•„ë“¤ëŸ¬(Alfred Adler)ì˜ ê°œì¸ì‹¬ë¦¬í•™ì„ ë”°ë¥´ëŠ” ì‹¬ë¦¬í•™ìì…ë‹ˆë‹¤.

í•µì‹¬ ì›ì¹™:
1. ì—´ë“±ê°ê³¼ ë³´ìƒ: ëª¨ë“  ì¸ê°„ì€ ì—´ë“±ê°ì„ ëŠë¼ë©°, ì´ë¥¼ ê·¹ë³µí•˜ë ¤ëŠ” ìš°ì›”ì„± ì¶”êµ¬ê°€ ì„±ì¥ì˜ ë™ë ¥ì…ë‹ˆë‹¤.
2. ì‚¬íšŒì  ê´€ì‹¬: ì¸ê°„ì€ ë³¸ì§ˆì ìœ¼ë¡œ ì‚¬íšŒì  ì¡´ì¬ì´ë©°, ê³µë™ì²´ ê°ê°ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
3. ìƒí™œì–‘ì‹: ê°œì¸ì˜ ë…íŠ¹í•œ ìƒí™œì–‘ì‹ì´ í–‰ë™ê³¼ ì‚¬ê³ ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
4. ëª©ì ë¡ ì  ê´€ì : ê³¼ê±°ë³´ë‹¤ëŠ” ë¯¸ë˜ì˜ ëª©í‘œê°€ í˜„ì¬ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤.
5. ê²©ë ¤: ìš©ê¸°ë¥¼ ë¶ë‹ìš°ëŠ” ê²ƒì´ ì¹˜ë£Œì˜ í•µì‹¬ì…ë‹ˆë‹¤.

ë‹µë³€ ë°©ì‹:
- ì—´ë“±ê°ì„ ì¸ì •í•˜ê³  ì´ë¥¼ ì„±ì¥ì˜ ê¸°íšŒë¡œ ì¬í•´ì„
- ì‚¬íšŒì  ê´€ì‹¬ê³¼ ê³µë™ì²´ ê°ê° ê°•ì¡°
- ê°œì¸ì˜ ì°½ì¡°ì  í˜ê³¼ ì„ íƒ ëŠ¥ë ¥ ê°•ì¡°
- ê²©ë ¤ì™€ ìš©ê¸°ë¥¼ ì£¼ëŠ” í†¤
- ëª©í‘œ ì§€í–¥ì  ê´€ì  ì œì‹œ
- **ë°˜ë“œì‹œ 2-3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€**

ë§íˆ¬:
- ê²©ë ¤ì ì´ê³  í¬ë§ì ì¸ í‘œí˜„ ì‚¬ìš©
- "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~ì˜ ê¸°íšŒì…ë‹ˆë‹¤" ë“± ê¸ì •ì  í‘œí˜„
- ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸
- ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ìƒëµí•˜ê³  í•µì‹¬ë§Œ ì „ë‹¬"""
    
    # ì‚¬ìš©ì ì…ë ¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜
    def translate_to_english(self, text: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì„ ì˜ì–´ë¡œ ë²ˆì—­"""
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a translator. Translate the following text to English. Only output the translation, nothing else."},
                    {"role": "user", "content": text}
                ],
                temperature=0.3,
                max_tokens=500
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"[ê²½ê³ ] ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜
    
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (OpenAI)
    def create_query_embedding(self, query_text: str) -> List[float]:
        """OpenAI text-embedding-3-largeë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±"""
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-large",
                input=query_text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    # ì‚¬ìš©ì ì…ë ¥ ë¶„ë¥˜ í•¨ìˆ˜
    def classify_input(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ë¥˜
        Returns:
            - "adler": ì•„ë“¤ëŸ¬ ê´€ë ¨ ì§ˆë¬¸
            - "counseling": ìƒë‹´/ê°ì • ê´€ë ¨ ì§ˆë¬¸
            - "general": ì¼ë°˜ ì§ˆë¬¸
        """
        user_input_lower = user_input.lower()
        
        # ì•„ë“¤ëŸ¬ í‚¤ì›Œë“œ ì²´í¬
        if "ì•„ë“¤ëŸ¬" in user_input or "adler" in user_input_lower:
            return "adler"
        
        # ê°ì •/ìƒë‹´ í‚¤ì›Œë“œ ì²´í¬
        for keyword in self.counseling_keywords:
            if keyword in user_input_lower:
                return "counseling"
        
        return "general"
    
    
    # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ìƒë‹´ ì²­í¬ë¡œë¶€í„° ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    def retrieve_chunks(self, user_input: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """
        Vector DBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        Args:
            user_input: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ (ì˜ì–´)
            n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        """
        # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = self.create_query_embedding(user_input)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        retrieved_chunks = []
        if results['ids'] and results['ids'][0]:
            for i in range(len(results['ids'][0])):
                chunk = {
                    'id': results['ids'][0][i],
                    'text': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i] if 'distances' in results else None
                }
                retrieved_chunks.append(chunk)
        
        return retrieved_chunks
    
    # í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ë‹µë³€ ìƒì„± (RAG + Persona)
    def generate_response_with_persona(self, user_input: str, retrieved_chunks: List[Dict[str, Any]], mode: str = "adler") -> Dict[str, Any]:
        """
        í˜ë¥´ì†Œë‚˜ë¥¼ ì ìš©í•œ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ (ì›ë¬¸, í•œêµ­ì–´)
            retrieved_chunks: ê²€ìƒ‰ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            mode: "adler" ë˜ëŠ” "counseling"
        """
        # ê²€ìƒ‰ëœ ì²­í¬ê°€ ì—†ëŠ” ê²½ìš°
        if not retrieved_chunks:
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?",
                "used_chunks": [],
                "continue_conversation": True
            }
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        used_chunks = []
        
        for i, chunk in enumerate(retrieved_chunks[:3], 1):  # ìƒìœ„ 3ê°œ ì²­í¬ ì‚¬ìš©
            chunk_text = chunk['text']
            source = chunk['metadata'].get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            context_parts.append(f"[ìë£Œ {i}]\n{chunk_text}\n(ì¶œì²˜: {source})")
            used_chunks.append(f"{source}: {chunk_text[:50]}...")
        
        context = "\n\n".join(context_parts)
        
        # ì•„ë“¤ëŸ¬ í˜ë¥´ì†Œë‚˜ ì‚¬ìš©
        persona_prompt = self.adler_persona
        user_message = f"""ì°¸ê³  ìë£Œ:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë“¤ëŸ¬ ê°œì¸ì‹¬ë¦¬í•™ ê´€ì ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ê²©ë ¤ì™€ ìš©ê¸°ë¥¼ ì£¼ëŠ” í†¤ìœ¼ë¡œ, ì—´ë“±ê°ì„ ì„±ì¥ì˜ ê¸°íšŒë¡œ ì¬í•´ì„í•˜ê³  ì‚¬íšŒì  ê´€ì‹¬ì„ ê°•ì¡°í•´ì£¼ì„¸ìš”.

**ì¤‘ìš”: ë‹µë³€ì€ 2-3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.**"""
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ë‹¨ê¸° ê¸°ì–µ)
        messages = [{"role": "system", "content": persona_prompt}]
        
        # ìµœê·¼ 2ê°œì˜ ëŒ€í™”ë§Œ í¬í•¨ (ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê´€ë¦¬)
        for history in self.chat_history[-2:]:
            messages.append({"role": "user", "content": history["user"]})
            messages.append({"role": "assistant", "content": history["assistant"]})
        
        messages.append({"role": "user", "content": user_message})
        
        # OpenAI API í˜¸ì¶œ
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.7,
                max_tokens=200  # ë‹µë³€ ê¸¸ì´ ì œí•œ (1000 -> 200)
            )
            
            answer = response.choices[0].message.content.strip()
            
            return {
                "answer": answer,
                "used_chunks": used_chunks,
                "mode": mode,
                "continue_conversation": True
            }
        
        except Exception as e:
            print(f"[ì˜¤ë¥˜] OpenAI ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "used_chunks": [],
                "mode": mode,
                "continue_conversation": True
            }
    
    # ìƒë‹´ í•¨ìˆ˜ 
    def chat(self, user_input: str) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        """
        # exit ì…ë ¥ í™•ì¸
        if user_input.strip().lower() == "exit":
            return {
                "answer": "ìƒë‹´ì„ ë§ˆë¬´ë¦¬í•˜ê² ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì–¸ì œë“  ë‹¤ì‹œ ì°¾ì•„ì£¼ì„¸ìš”.",
                "used_chunks": [],
                "mode": "exit",
                "continue_conversation": False
            }
        
        # 1. ì…ë ¥ ë¶„ë¥˜
        input_type = self.classify_input(user_input)
        mode_name = {"adler": "ì•„ë“¤ëŸ¬ ëª¨ë“œ", "counseling": "ìƒë‹´ ëª¨ë“œ", "general": "ì¼ë°˜ ëª¨ë“œ"}
        print(f"\nğŸ“‹ ì…ë ¥ ìœ í˜•: {mode_name.get(input_type, input_type)}")
        
        # 2. ì˜ì–´ë¡œ ë²ˆì—­ (Vector DB ê²€ìƒ‰ìš©)
        print("ğŸŒ ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘...")
        english_input = self.translate_to_english(user_input)
        print(f"âœ“ ë²ˆì—­ ì™„ë£Œ: {english_input[:50]}...")
        
        # 3. ì…ë ¥ ìœ í˜•ì— ë”°ë¥¸ ì²˜ë¦¬ (ëª¨ë“  ëª¨ë“œì—ì„œ ì•„ë“¤ëŸ¬ í˜ë¥´ì†Œë‚˜ ì‚¬ìš©)
        print("\nğŸ” ê´€ë ¨ ìë£Œ ê²€ìƒ‰ ì¤‘...")
        retrieved_chunks = self.retrieve_chunks(english_input, n_results=5)
        print(f"âœ“ {len(retrieved_chunks)}ê°œì˜ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ­ ì•„ë“¤ëŸ¬ í˜ë¥´ì†Œë‚˜ ì ìš© ì¤‘...\n")
        
        response = self.generate_response_with_persona(user_input, retrieved_chunks, mode=input_type)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ë‹¨ê¸° ê¸°ì–µ)
        self.chat_history.append({
            "user": user_input,
            "assistant": response["answer"]
        })
        
        # íˆìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì˜¤ë˜ëœ ê²ƒ ì œê±° (ìµœëŒ€ 10ê°œ ìœ ì§€)
        if len(self.chat_history) > 10:
            self.chat_history = self.chat_history[-10:]
        
        return response

# ë©”ì¸
def main():
    
    # console ì¶œë ¥ ìš©ë„, ë‚˜ì¤‘ì— ì‚­ì œ ì˜ˆì •
    print("=" * 70)
    print("RAG ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ")
    print("=" * 70)
    
    # ê²½ë¡œ ì„¤ì • (sourcecode/rag ê¸°ì¤€)
    base_dir = Path(__file__).parent.parent.parent
    vector_db_dir = base_dir / "vector_db"
    
    try:
        # RAG ìƒë‹´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\nì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        print("- OpenAI API ì—°ê²° í™•ì¸")
        print("- Vector DB ë¡œë“œ")
        rag_system = RAGTherapySystem(str(vector_db_dir))
        print("âœ“ ì´ˆê¸°í™” ì™„ë£Œ\n")
        
        # ìƒë‹´ ì‹œì‘
        print("\n" + "=" * 70)
        print("ğŸ­ ì•„ë“¤ëŸ¬ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ RAG ìƒë‹´ ì‹œìŠ¤í…œ")
        print("=" * 70)
        print("\nğŸ’¬ ëŒ€í™” ëª¨ë“œ:")
        print("  â€¢ ì•„ë“¤ëŸ¬ ëª¨ë“œ: 'ì•„ë“¤ëŸ¬' í‚¤ì›Œë“œ í¬í•¨ ì‹œ")
        print("  â€¢ ìƒë‹´ ëª¨ë“œ: ê°ì •/ê³ ë¯¼ í‘œí˜„ ì‹œ")
        print("  â€¢ ì¼ë°˜ ëª¨ë“œ: ê¸°íƒ€ ì§ˆë¬¸")
        print("  (ëª¨ë“  ëª¨ë“œì—ì„œ ì•„ë“¤ëŸ¬ í˜ë¥´ì†Œë‚˜ ì ìš©)")
        print("\nâœ¨ íŠ¹ì§•:")
        print("  â€¢ ë‹¤êµ­ì–´ ìë™ ë²ˆì—­ ì§€ì›")
        print("  â€¢ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ë§¥ë½ ìœ ì§€ (ë‹¨ê¸° ê¸°ì–µ)")
        print("  â€¢ ì•„ë“¤ëŸ¬ ê°œì¸ì‹¬ë¦¬í•™ ê¸°ë°˜ ë‹µë³€")
        print("\nì¢…ë£Œí•˜ì‹œë ¤ë©´ 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 70)
        
        # ëŒ€í™” ë£¨í”„
        while True:
            print("\n" + "-" * 70)
            user_input = input("\n[ì‚¬ìš©ì] ").strip()
            
            if not user_input:
                print("[ì•Œë¦¼] ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ìƒë‹´ ì§„í–‰
            response = rag_system.chat(user_input)
            
            # ë‹µë³€ ì¶œë ¥
            print(f"\n[ğŸ­ ì•„ë“¤ëŸ¬ ìƒë‹´ì‚¬]")
            print(response['answer'])
            
            # ì‚¬ìš©ëœ ì²­í¬ ì •ë³´ (ë””ë²„ê¹…ìš©, í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            if response.get('used_chunks'):
                print("\n[ğŸ“š ì°¸ê³ í•œ ìë£Œ]")
                for i, chunk in enumerate(response['used_chunks'], 1):
                    print(f"  {i}. {chunk}")
            
            # ì¢…ë£Œ í™•ì¸
            if not response['continue_conversation']:
                break
        
        print("\n" + "=" * 70)
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("=" * 70)
    
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

