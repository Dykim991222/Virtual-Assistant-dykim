"""
Persona ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ
ìƒì„±ë‚ ì§œ: 2025.11.18
ì„¤ëª…: ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ ì§ˆë¬¸ì¸ì§€ ìƒë‹´ì¸ì§€ íŒë‹¨í•˜ê³ , 
      RAG ì—”ì§„ì—ì„œ ê²€ìƒ‰ëœ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹¼ ë¡œì €ìŠ¤ ìŠ¤íƒ€ì¼ì˜ Personaë¥¼ ì…í˜€ ë‹µë³€ì„ ìƒì„±
"""

import json
import chromadb
from pathlib import Path
from typing import List, Dict, Any
import torch
from transformers import AutoModel, AutoTokenizer
import re


# ì¹¼ ë¡œì €ìŠ¤ ìƒë‹´ ìŠ¤íƒ€ì¼ Persona ì§€ì¹¨
PERSONA_INSTRUCTIONS = """
ë‹¹ì‹ ì€ ì¹¼ ë¡œì €ìŠ¤(Carl Rogers)ì˜ ì¸ê°„ì¤‘ì‹¬ ìƒë‹´ ì´ë¡ ì„ ë”°ë¥´ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

í•µì‹¬ ì›ì¹™:
1. ë¬´ì¡°ê±´ì  ê¸ì •ì  ì¡´ì¤‘ (Unconditional Positive Regard): ì‚¬ìš©ìë¥¼ ìˆëŠ” ê·¸ëŒ€ë¡œ ìˆ˜ìš©í•˜ê³  ì¡´ì¤‘í•©ë‹ˆë‹¤.
2. ê³µê°ì  ì´í•´ (Empathic Understanding): ì‚¬ìš©ìì˜ ê°ì •ê³¼ ê²½í—˜ì„ ê¹Šì´ ì´í•´í•˜ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤.
3. ì§„ì†”ì„± (Genuineness): ì§„ì‹¤ë˜ê³  ì¼ì¹˜ëœ íƒœë„ë¡œ ëŒ€í•©ë‹ˆë‹¤.

ìƒë‹´ ë°©ì‹:
- Reflective Listening: ì‚¬ìš©ìì˜ ë§ì„ ë°˜ì˜í•˜ê³  ì¬ì§„ìˆ í•©ë‹ˆë‹¤
- ê°ì • ê³µê°: ì‚¬ìš©ìì˜ ê°ì •ì„ ë¨¼ì € ì¸ì‹í•˜ê³  ê³µê°í•©ë‹ˆë‹¤
- ë¹„ì§€ì‹œì  ì ‘ê·¼: ì¡°ì–¸ë³´ë‹¤ëŠ” ì‚¬ìš©ì ìŠ¤ìŠ¤ë¡œ ë‹µì„ ì°¾ë„ë¡ ë•ìŠµë‹ˆë‹¤
- ë”°ëœ»í•˜ê³  ìˆ˜ìš©ì ì¸ í†¤: íŒë‹¨í•˜ì§€ ì•Šê³  ë”°ëœ»í•˜ê²Œ ëŒ€í•©ë‹ˆë‹¤

ë§íˆ¬:
- "~í•˜ì‹œëŠ”êµ°ìš”", "~í•˜ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”" ë“± ë¶€ë“œëŸ½ê³  ì¡´ì¤‘í•˜ëŠ” í‘œí˜„ ì‚¬ìš©
- "ì œê°€ ì´í•´í•œ ë°”ë¡œëŠ”..." ë“±ìœ¼ë¡œ reflective listening í‘œí˜„
- ì§§ê³  ê°„ê²°í•˜ê²Œ, ê·¸ëŸ¬ë‚˜ ë”°ëœ»í•˜ê²Œ
"""


# Persona ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ
class PersonaTherapySystem:
    
    # ì´ˆê¸°í™” í•¨ìˆ˜
    def __init__(self, vector_db_path: str, model_name: str = "BAAI/bge-m3"):

        # Vector DB ê²½ë¡œ ì„¤ì •
        self.db_path = Path(vector_db_path)
        
        # Vector DB ì¡´ì¬ í™•ì¸
        if not self.db_path.exists():
            raise FileNotFoundError(f"Vector DB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.db_path}")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path=str(self.db_path))
        
        # ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
        self.collection_map = {
            1: "paragraph_vec",  # ë¬¸ë‹¨ ê¸°ë°˜
            2: "semantic_vec"    # ì˜ë¯¸ ê¸°ë°˜
        }
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU/CPU)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        # í˜„ì¬ ì„ íƒëœ ì»¬ë ‰ì…˜
        self.current_collection = None
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.chat_history = []
        
        # Persona ì§€ì¹¨
        self.persona_instructions = PERSONA_INSTRUCTIONS
    
    # í‰ê·  í’€ë§ í•¨ìˆ˜
    def mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    def create_query_embedding(self, query_text: str) -> List[float]:

        # í† í°í™”
        encoded_input = self.tokenizer(
            query_text,
            padding=True,
            truncation=True,
            return_tensors='pt',
            max_length=512
        )                  
        
        # GPU/CPUë¡œ ì´ë™
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            model_output = self.model(**encoded_input)
            embedding = self.mean_pooling(model_output, encoded_input['attention_mask'])
            embedding = embedding.cpu().numpy()[0]
        
        return embedding.tolist()
    
    # ì–´ë–¤ Vector DBë¥¼ ì‚¬ìš©í• ê±´ì§€ ì„ íƒí•˜ëŠ” í•¨ìˆ˜
    def select_collection(self, db_choice: int) -> bool:

        # ë§Œì•½ ì˜ëª»ëœ ê°’ ì…ë ¥ ì‹œ ì˜ˆì™¸ì²˜ë¦¬
        if db_choice not in self.collection_map:
            print(f"[ì˜¤ë¥˜] ì˜ëª»ëœ DB ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return False
        
        # ì„ íƒí•œ DB ì´ë¦„ì„ ë³€ìˆ˜ì— ì €ì¥
        collection_name = self.collection_map[db_choice]
        
        try:
            self.current_collection = self.client.get_collection(name=collection_name)
            collection_type = "ë¬¸ë‹¨ ê¸°ë°˜" if db_choice == 1 else "ì˜ë¯¸ ê¸°ë°˜"
            print(f"\nâœ“ '{collection_type}' Vector DB ì„ íƒ ì™„ë£Œ (ì»¬ë ‰ì…˜: {collection_name})")
            return True
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {collection_name}")
            print(f"ìƒì„¸: {e}")
            return False
    
    # ì…ë ¥ì´ ì§ˆë¬¸ì¸ì§€ ìƒë‹´ì¸ì§€ ìë™ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜ (Rule 2 - ìë™ íŒë‹¨)
    def classify_input_type_auto(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì´ ì§ˆë¬¸ì¸ì§€ ìƒë‹´ì¸ì§€ ìë™ìœ¼ë¡œ íŒë‹¨
        ê°ì • í‘œí˜„, ê³ ë¯¼, ì‹¬ë¦¬ì  ì–´ë ¤ì›€ì´ í¬í•¨ë˜ë©´ 'counseling'
        ì •ë³´ ìš”ì²­, ì‚¬ì‹¤ í™•ì¸ ë“±ì€ 'question'
        """
        # ê°ì •/ê³ ë¯¼ í‚¤ì›Œë“œ (ìƒë‹´ íŒë‹¨ìš©)
        counseling_patterns = [
            r'í˜ë“¤', r'ì–´ë µ', r'ê´´ë¡­', r'ìš°ìš¸', r'ë¶ˆì•ˆ', r'ìŠ¬í”„', r'í™”ê°€', r'ì™¸ë¡œ', r'ë¬´ì„­',
            r'ê³ ë¯¼', r'ê±±ì •', r'ë‘ë µ', r'ë‹µë‹µ', r'ì†ìƒ', r'ì§œì¦', r'ìŠ¤íŠ¸ë ˆìŠ¤', r'í”¼ê³¤',
            r'ì§€ì³', r'ì¢Œì ˆ', r'ì‹¤ë§', r'í›„íšŒ', r'ë¯¸ì•ˆ', r'ì£„ì±…ê°',
            r'ì–´ë–»ê²Œ í•´ì•¼', r'ì¡°ì–¸', r'ë„ì›€', r'ìƒë‹´', r'ì´ì•¼ê¸° ë“¤ì–´',
            r'ë‚˜ëŠ”.*ëŠë‚€ë‹¤', r'ë‚˜ëŠ”.*ìƒê°í•œë‹¤', r'ë‚´ê°€.*í•˜ëŠ”ë°'
        ]
        
        # ì§ˆë¬¸ íŒ¨í„´ (ì§ˆë¬¸ íŒë‹¨ìš©)
        question_patterns = [
            r'ë¬´ì—‡', r'ë¬´ìŠ¨', r'ì–´ë–¤', r'ì–¸ì œ', r'ì–´ë””', r'ëˆ„ê°€', r'ì™œ', r'ì–´ë–»ê²Œ',
            r'ì´ë€', r'ë€', r'ì´ë‹¤', r'ì¸ê°€', r'ì…ë‹ˆê¹Œ',
            r'\?', r'ì•Œë ¤', r'ì„¤ëª…', r'ì •ì˜', r'ì˜ë¯¸', r'ì°¨ì´', r'ë°©ë²•'
        ]
        
        # ìƒë‹´ íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜
        counseling_score = sum(1 for pattern in counseling_patterns if re.search(pattern, user_input))
        
        # ì§ˆë¬¸ íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜
        question_score = sum(1 for pattern in question_patterns if re.search(pattern, user_input))
        
        # ì ìˆ˜ ê¸°ë°˜ íŒë‹¨
        if counseling_score > question_score:
            return "counseling"
        elif question_score > counseling_score:
            return "question"
        else:
            # ë™ì ì´ê±°ë‚˜ ë‘˜ ë‹¤ 0ì¸ ê²½ìš°, ë¬¸ì¥ ê¸¸ì´ì™€ êµ¬ì¡°ë¡œ íŒë‹¨
            # ê¸´ ë¬¸ì¥ì´ê³  ê°ì • í‘œí˜„ì´ ìˆìœ¼ë©´ ìƒë‹´ìœ¼ë¡œ ê°„ì£¼
            if len(user_input) > 30 and any(char in user_input for char in ['..', '...']):
                return "counseling"
            # ì§§ê³  ë¬¼ìŒí‘œê°€ ìˆìœ¼ë©´ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼
            elif '?' in user_input or len(user_input) < 30:
                return "question"
            else:
                # ê¸°ë³¸ê°’ì€ ìƒë‹´ìœ¼ë¡œ ê°„ì£¼ (ì•ˆì „í•œ ì„ íƒ)
                return "counseling"
    
    # ì…ë ¥ì´ ì§ˆë¬¸ì¸ì§€ ìƒë‹´ì¸ì§€ í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜ (Rule 2 - í‚¤ì›Œë“œ ê¸°ë°˜, ì£¼ì„ì²˜ë¦¬)
    """
    def classify_input_type_keyword(self, user_input: str) -> str:
        
        # ìƒë‹´ í‚¤ì›Œë“œ
        counseling_keywords = [
            'í˜ë“¤ì–´', 'ì–´ë ¤ì›Œ', 'ê´´ë¡œì›Œ', 'ìš°ìš¸', 'ë¶ˆì•ˆ', 'ìŠ¬í¼', 'í™”ê°€', 'ì™¸ë¡œì›Œ',
            'ê³ ë¯¼', 'ê±±ì •', 'ë‘ë ¤ì›Œ', 'ë‹µë‹µ', 'ì†ìƒ', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ì§€ì³', 'ìƒë‹´'
        ]
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ
        question_keywords = [
            'ë¬´ì—‡', 'ë¬´ìŠ¨', 'ì–´ë–¤', 'ì–¸ì œ', 'ì–´ë””', 'ì™œ', 'ì–´ë–»ê²Œ',
            'ì•Œë ¤', 'ì„¤ëª…', 'ì •ì˜', 'ì˜ë¯¸', 'ì°¨ì´', 'ë°©ë²•'
        ]
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        is_counseling = any(keyword in user_input for keyword in counseling_keywords)
        is_question = any(keyword in user_input for keyword in question_keywords)
        
        if is_counseling and not is_question:
            return "counseling"
        elif is_question and not is_counseling:
            return "question"
        else:
            # ì• ë§¤í•œ ê²½ìš° ë¬¸ì¥ êµ¬ì¡°ë¡œ íŒë‹¨
            if '?' in user_input or user_input.endswith('ì¸ê°€') or user_input.endswith('ì¸ê°€ìš”'):
                return "question"
            else:
                return "counseling"
    """
    
    # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ìƒë‹´ ì²­í¬ë¡œë¶€í„° ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    def retrieve_chunks(self, user_input: str, n_results: int = 5) -> List[Dict[str, Any]]:

        if self.current_collection is None:
            print("[ì˜¤ë¥˜] Vector DBê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = self.create_query_embedding(user_input)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = self.current_collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        retrieved_chunks = []
        if results['ids'] and results['ids'][0]:
            for i in range(len(results['ids'][0])):
                chunk = {
                    'id': results['ids'][0][i],
                    'text': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i] if 'distances' in results else None
                }
                retrieved_chunks.append(chunk)
        
        return retrieved_chunks
    
    # RAG ì—”ì§„ì—ì„œ ê¸°ë³¸ ë‹µë³€ ìƒì„± (Persona ì ìš© ì „)
    def generate_rag_answer(self, user_input: str, retrieved_chunks: List[Dict[str, Any]]) -> str:
        """
        RAG ì—”ì§„ì—ì„œ ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ë‹µë³€ ìƒì„±
        """
        if not retrieved_chunks:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ìƒìœ„ 3ê°œ ì²­í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ êµ¬ì„±
        rag_answer_parts = []
        for i, chunk in enumerate(retrieved_chunks[:3], 1):
            chunk_text = chunk['text']
            rag_answer_parts.append(chunk_text)
        
        return "\n\n".join(rag_answer_parts)
    
    # ì§ˆë¬¸ íƒ€ì…ì— ë§ì¶° ìµœì¢… ë‹µë³€ ìƒì„± (Rule 3, 4)
    def generate_final_answer(
        self, 
        user_input: str, 
        input_type: str, 
        rag_answer: str, 
        retrieved_chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ì…ë ¥ íƒ€ì…ì— ë”°ë¼ ìµœì¢… ë‹µë³€ ìƒì„±
        - question: ì •í™•í•˜ê³  ê°„ê²°í•œ ì •ë³´ ì œê³µ
        - counseling: Persona ìŠ¤íƒ€ì¼ì„ ì…í˜€ ê³µê°ì  ìƒë‹´ ì§„í–‰
        """
        
        # Rule 3: ì§ˆë¬¸ì¸ ê²½ìš° - ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€ (Rule 6: ì¥í™©í•œ ì„¤ëª… í”¼í•˜ê¸°)
        if input_type == "question":
            if not retrieved_chunks:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
            else:
                # RAG ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ ì œê³µ
                answer_parts = []
                answer_parts.append(f"ì§ˆë¬¸í•´ì£¼ì‹  '{user_input}'ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n")
                answer_parts.append(rag_answer)
                answer_parts.append("\n\në” ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.")
                answer = "".join(answer_parts)
        
        # Rule 4: ìƒë‹´ì¸ ê²½ìš° - Persona ìŠ¤íƒ€ì¼ ì ìš© (Rule 5: ê³µê°, reflective listening)
        else:  # counseling
            if not retrieved_chunks:
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ê³µê°ì ìœ¼ë¡œ ì‘ë‹µ
                answer = f"ë§ì”€í•´ì£¼ì‹  '{user_input}'ì— ëŒ€í•´ í•¨ê»˜ ìƒê°í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ëŠë¼ì‹œëŠ” ê°ì •ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ë“¤ë ¤ì£¼ì‹œê² ì–´ìš”?"
            else:
                # Persona ìŠ¤íƒ€ì¼ ì ìš©: ì¹¼ ë¡œì €ìŠ¤ ë°©ì‹ì˜ ê³µê°ì  ìƒë‹´
                answer_parts = []
                
                # 1. Reflective Listening: ì‚¬ìš©ì ë§ ë°˜ì˜
                answer_parts.append(f"ë§ì”€í•˜ì‹  ê²ƒì„ ë“¤ìœ¼ë‹ˆ, {self._extract_emotion(user_input)} ëŠë¼ê³  ê³„ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”.")
                
                # 2. ê³µê° í‘œí˜„
                answer_parts.append(" ê·¸ëŸ° ìƒí™©ì´ë¼ë©´ ì¶©ë¶„íˆ ê·¸ë ‡ê²Œ ëŠë¼ì‹¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
                
                # 3. RAG ê¸°ë°˜ ìƒë‹´ ë‚´ìš© ì œê³µ (ë¶€ë“œëŸ½ê³  ìˆ˜ìš©ì ì¸ í†¤ìœ¼ë¡œ)
                answer_parts.append("\n\nì œê°€ ì´í•´í•œ ë°”ë¡œëŠ”, ì´ëŸ° ê´€ì ë„ ë„ì›€ì´ ë  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤:\n")
                
                # ì²­í¬ ë‚´ìš©ì„ ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ì¬êµ¬ì„±
                for i, chunk in enumerate(retrieved_chunks[:2], 1):  # ìƒìœ„ 2ê°œë§Œ ì‚¬ìš© (ê°„ê²°í•˜ê²Œ)
                    chunk_text = chunk['text']
                    # ì²­í¬ë¥¼ ë¶€ë“œëŸ½ê²Œ ì¬í‘œí˜„
                    answer_parts.append(f"\n{chunk_text}")
                
                # 4. ë§ˆë¬´ë¦¬: ë¹„ì§€ì‹œì  ì ‘ê·¼ (ì¡°ì–¸ë³´ë‹¤ëŠ” íƒìƒ‰ ìœ ë„)
                answer_parts.append("\n\nì´ëŸ° ì´ì•¼ê¸°ë“¤ì´ ì§€ê¸ˆì˜ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ëŠê»´ì§€ì‹œë‚˜ìš”? ë” ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹  ì´ì•¼ê¸°ê°€ ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.")
                
                answer = "".join(answer_parts)
        
        # Rule 6: chat_historyë¥¼ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        if self.chat_history:
            # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì´ì „ ëŒ€í™”ì™€ ì—°ê²°
            answer = self._add_history_context(answer)
        
        return {
            "input_type": input_type,
            "answer": answer,
            "used_rag_answer": rag_answer[:100] + "..." if len(rag_answer) > 100 else rag_answer,
            "continue_conversation": True
        }
    
    # ê°ì • ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜ (Rule 5: ê°ì • ê³µê°)
    def _extract_emotion(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì—ì„œ ê°ì •ì„ ì¶”ì¶œí•˜ì—¬ reflective listeningì— í™œìš©
        """
        emotion_keywords = {
            'í˜ë“¤': 'í˜ë“œì‹œê²Œ',
            'ì–´ë ¤': 'ì–´ë ¤ìš°ì‹œê²Œ',
            'ê´´ë¡­': 'ê´´ë¡œìš°ì‹œê²Œ',
            'ìš°ìš¸': 'ìš°ìš¸í•˜ì‹œê²Œ',
            'ë¶ˆì•ˆ': 'ë¶ˆì•ˆí•˜ì‹œê²Œ',
            'ìŠ¬í”„': 'ìŠ¬í”„ì‹œê²Œ',
            'í™”ê°€': 'í™”ê°€ ë‚˜ì‹œê²Œ',
            'ì™¸ë¡œ': 'ì™¸ë¡œìš°ì‹œê²Œ',
            'ê±±ì •': 'ê±±ì •ë˜ì‹œê²Œ',
            'ë‘ë µ': 'ë‘ë ¤ìš°ì‹œê²Œ',
            'ë‹µë‹µ': 'ë‹µë‹µí•˜ì‹œê²Œ',
            'ì†ìƒ': 'ì†ìƒí•˜ì‹œê²Œ',
            'ìŠ¤íŠ¸ë ˆìŠ¤': 'ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ì‹œê²Œ',
            'ì§€ì³': 'ì§€ì¹˜ì‹œê²Œ',
        }
        
        for keyword, emotion in emotion_keywords.items():
            if keyword in user_input:
                return emotion
        
        # ê¸°ë³¸ ê°ì • í‘œí˜„
        return "ì—¬ëŸ¬ ê°ì •ì„"
    
    # ëŒ€í™” ê¸°ë¡ ê¸°ë°˜ ë§¥ë½ ì¶”ê°€ (Rule 6)
    def _add_history_context(self, answer: str) -> str:
        """
        ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì— ë§¥ë½ì„ ì¶”ê°€
        """
        # ê°„ë‹¨í•œ ì˜ˆì‹œ: ì´ì „ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ì—°ê²° í‘œí˜„ ì¶”ê°€
        if len(self.chat_history) > 0:
            last_exchange = self.chat_history[-1]
            # í•„ìš”ì‹œ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ ìˆ˜ì •
            # í˜„ì¬ëŠ” ë‹¨ìˆœíˆ ì—°ê²° í‘œí˜„ë§Œ ì¶”ê°€
            pass
        
        return answer
    
    # ë©”ì¸ ìƒë‹´ í•¨ìˆ˜
    def chat(self, user_input: str) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì „ì²´ ìƒë‹´ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
        """
        
        # Rule 9: exit ì…ë ¥ í™•ì¸
        if user_input.strip().lower() == "exit":
            return {
                "input_type": "exit",
                "answer": "ìƒë‹´ì„ ë§ˆë¬´ë¦¬í•˜ê² ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì–¸ì œë“  ë‹¤ì‹œ ì°¾ì•„ì£¼ì„¸ìš”.",
                "used_rag_answer": "",
                "continue_conversation": False
            }
        
        # Rule 1: ì…ë ¥ íƒ€ì… ìë™ íŒë‹¨
        input_type = self.classify_input_type_auto(user_input)
        print(f"\nğŸ” ì…ë ¥ ë¶„ì„: {'ìƒë‹´' if input_type == 'counseling' else 'ì§ˆë¬¸'} ëª¨ë“œ")
        
        # RAG ì—”ì§„: ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        print("ğŸ” ê´€ë ¨ ìë£Œ ê²€ìƒ‰ ì¤‘...")
        retrieved_chunks = self.retrieve_chunks(user_input, n_results=5)
        print(f"âœ“ {len(retrieved_chunks)}ê°œì˜ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
        
        # RAG ê¸°ë³¸ ë‹µë³€ ìƒì„±
        rag_answer = self.generate_rag_answer(user_input, retrieved_chunks)
        
        # ìµœì¢… ë‹µë³€ ìƒì„± (ì…ë ¥ íƒ€ì…ì— ë”°ë¼ Persona ì ìš©)
        response = self.generate_final_answer(
            user_input=user_input,
            input_type=input_type,
            rag_answer=rag_answer,
            retrieved_chunks=retrieved_chunks
        )
        
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        self.chat_history.append({
            "user": user_input,
            "assistant": response["answer"],
            "type": input_type
        })
        
        return response


# ë©”ì¸ í•¨ìˆ˜
def main():
    
    print("=" * 70)
    print("Persona ê¸°ë°˜ ìƒë‹´ ì‹œìŠ¤í…œ (ì¹¼ ë¡œì €ìŠ¤ ìŠ¤íƒ€ì¼)")
    print("=" * 70)
    
    # ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).parent.parent.parent
    vector_db_dir = base_dir / "vector_db"
    
    try:
        # Persona ìƒë‹´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\nì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n")
        persona_system = PersonaTherapySystem(str(vector_db_dir))
        
        # DB ì„ íƒ
        print("\n" + "=" * 70)
        print("Vector DB ì„ íƒ")
        print("=" * 70)
        print("1. ë¬¸ë‹¨ ê¸°ë°˜ (paragraph_vec)")
        print("2. ì˜ë¯¸ ê¸°ë°˜ (semantic_vec)")
        print("=" * 70)
        
        while True:
            db_choice = input("\nDB ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()
            
            if db_choice in ['1', '2']:
                if persona_system.select_collection(int(db_choice)):
                    break
            else:
                print("[ì˜¤ë¥˜] 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ìƒë‹´ ì‹œì‘
        print("\n" + "=" * 70)
        print("ìƒë‹´ ì‹œì‘")
        print("=" * 70)
        print("ì§ˆë¬¸ì´ë‚˜ ê³ ë¯¼ì„ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.")
        print("ì¢…ë£Œí•˜ì‹œë ¤ë©´ 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 70)
        
        # ëŒ€í™” ë£¨í”„
        while True:
            print("\n" + "-" * 70)
            user_input = input("\n[ì‚¬ìš©ì] ").strip()
            
            if not user_input:
                print("[ì•Œë¦¼] ì§ˆë¬¸ì´ë‚˜ ê³ ë¯¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ìƒë‹´ ì§„í–‰
            response = persona_system.chat(user_input)
            
            # ë‹µë³€ ì¶œë ¥
            print("\n[ìƒë‹´ì‚¬]")
            print(response['answer'])
            
            # ë””ë²„ê¹… ì •ë³´ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # print(f"\n[DEBUG] ì…ë ¥ íƒ€ì…: {response['input_type']}")
            # print(f"[DEBUG] RAG ë‹µë³€ ìš”ì•½: {response['used_rag_answer']}")
            
            # ì¢…ë£Œ í™•ì¸
            if not response['continue_conversation']:
                break
        
        print("\n" + "=" * 70)
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("=" * 70)
    
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

